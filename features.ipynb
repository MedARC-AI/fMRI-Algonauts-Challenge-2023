{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22895cd8",
   "metadata": {},
   "source": [
    "# Import packages + functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712b213c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "from subprocess import call\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc04fa1",
   "metadata": {},
   "source": [
    "# Load NSD webdatasets for a given subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6ef00",
   "metadata": {},
   "source": [
    "The '/fsx' directory refers to a directory housed on the Stability HPC. You will need to download NSD data from [our huggingface dataset](https://huggingface.co/datasets/pscotti/naturalscenesdataset/tree/main) and change the folders below to your own folder path. If you have demonstrated serious involvement with the project and need access to the Stability HPC contact Paul Scotti on Discord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbc9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # alexnet requirement\n",
    "])\n",
    "\n",
    "def preprocess(sample):\n",
    "    voxel, image = sample\n",
    "    return voxel, transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67101d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_devices 1\n",
      "num_workers 1\n",
      "batch_size 300\n",
      "global_batch_size 300\n",
      "num_worker_batches 10\n",
      "validation: num_worker_batches 1\n"
     ]
    }
   ],
   "source": [
    "subject = '01'\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: # not using gpu, assuming you want to use the limited dataset\n",
    "    num_devices = 1\n",
    "    num_workers = 1\n",
    "    batch_size = 300\n",
    "    num_samples = 9000\n",
    "else: # using cuda, assuming you want to use the full dataset\n",
    "    num_workers = num_devices\n",
    "    batch_size = 300\n",
    "    num_samples = 3000 # see metadata.json\n",
    "print(\"num_devices\",num_devices)\n",
    "print(\"num_workers\",num_workers)\n",
    "print(\"batch_size\",batch_size)\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"global_batch_size\",global_batch_size)\n",
    "num_batches = math.floor(num_samples / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "print(\"num_worker_batches\",num_worker_batches)\n",
    "# train_url = f'/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/train/train_subj{subject}'\n",
    "train_url = f'/scratch/gpfs/KNORMAN/webdataset_nsd/webdataset_split/train/train_subj{subject}'\n",
    "train_url = train_url+\"_{0..49}.tar\"\n",
    "\n",
    "train_data = wds.DataPipeline([wds.ResampledShards(train_url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    #wds.shuffle(500,initial=500),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", 'images'),\n",
    "                    wds.map(preprocess),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=None, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Validation #\n",
    "# num_samples = 492\n",
    "num_samples = 300\n",
    "num_batches = math.ceil(num_samples / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "print(\"validation: num_worker_batches\",num_worker_batches)\n",
    "\n",
    "# val_url = f'/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/val/val_subj{subject}_0.tar'\n",
    "val_url = f'/scratch/gpfs/KNORMAN/webdataset_nsd/webdataset_split/val/val_subj{subject}_0.tar'\n",
    "val_data = wds.DataPipeline([wds.SimpleShardList(val_url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", 'images'),\n",
    "                    wds.map(preprocess),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7cbd6b",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([300, 15724]) tensor([0.7324, 0.0721, 0.0433, 1.4102], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([-0.3198, -0.3369, -0.3198, -0.3027])\n",
      "idx 1\n",
      "voxel.shape torch.Size([300, 15724]) tensor([1.0791, 1.2344, 1.5664, 1.1289], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([-1.2274, -1.2617, -1.2959, -1.2617])\n",
      "idx 2\n",
      "voxel.shape torch.Size([300, 15724]) tensor([-0.3252, -0.4446,  0.6084, -0.1917], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([2.2489, 2.2489, 2.2489, 2.2489])\n",
      "idx 3\n",
      "voxel.shape torch.Size([300, 15724]) tensor([-0.0909,  0.5122,  0.4985,  0.8706], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([0.9474, 0.9646, 0.9474, 0.9474])\n"
     ]
    }
   ],
   "source": [
    "# check that your training data loaders are working\n",
    "for train_i, (voxel, img_input) in enumerate(train_dl):\n",
    "    print(\"idx\",train_i)\n",
    "    print(\"voxel.shape\",voxel.shape,voxel[0,20:24])\n",
    "    print(\"img_input.shape\",img_input.shape,img_input[0,0,0,100:104])\n",
    "    if train_i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf574d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([300, 15724]) tensor([ 1.0420,  1.2803, -0.4688,  1.0654], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([-0.1999, -0.0972, -0.0801, -0.0458])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check that validation data loader is working\n",
    "for val_i, (voxel, img_input) in enumerate(tqdm(val_dl)):\n",
    "    print(\"idx\",val_i)\n",
    "    print(\"voxel.shape\",voxel.shape,voxel[0,20:24])\n",
    "    print(\"img_input.shape\",img_input.shape,img_input[0,0,0,100:104])\n",
    "    if train_i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bfa13c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# check validation batches are working correctly\n",
    "# nothing prints out if correct\n",
    "for train_i, (voxel, img_input) in enumerate(tqdm(val_dl)):\n",
    "    if voxel.shape[0]!=batch_size:\n",
    "        print(\"idx\",train_i)\n",
    "        print(\"voxel.shape\",voxel.shape,voxel[0,20:24])\n",
    "        print(\"img_input.shape\",img_input.shape,img_input[0,0,0,100:104])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40874f",
   "metadata": {},
   "source": [
    "# Train model (input = image; output = predicted fMRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6cc76",
   "metadata": {},
   "source": [
    "### Extract and downsample chosen alexnet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72a0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental PCA computes features\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=100, batch_size=batch_size)\n",
    "    # Fit PCA to batch\n",
    "    for _, (voxel, img) in enumerate(tqdm(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(img.to(device))\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9fd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets principal components of features for each images using derived PCA\n",
    "# while going through dataset, also prepare training/validation fMRI matrix from dataloader for regression\n",
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    fmri = torch.tensor([])\n",
    "    for _, (voxel, img) in enumerate(tqdm(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(img.to(device))\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "        \n",
    "        fmri = torch.cat((fmri, voxel), 0)\n",
    "    return np.vstack(features), fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319a6aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 104, 83])\n",
      "tensor(15724)\n"
     ]
    }
   ],
   "source": [
    "### mask for nsdgeneral voxels\n",
    "\n",
    "# whole brain space is 81x104x83\n",
    "# nsdgeneral sapce is 15724\n",
    "import nibabel as nib\n",
    "\n",
    "file = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/subj{subject}/func1pt8mm/roi/nsdgeneral.nii.gz\"\n",
    "nifti = nib.load(file)\n",
    "nsd_mask = nifti.get_fdata()\n",
    "nsd_mask = nsd_mask>0\n",
    "nsd_mask = torch.from_numpy(nsd_mask)\n",
    "print(nsd_mask.shape)\n",
    "print(torch.sum(nsd_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebde86c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 104, 83])\n",
      "tensor(15724)\n"
     ]
    }
   ],
   "source": [
    "# whole brain space is 81x104x83\n",
    "# nsdgeneral sapce is 15724\n",
    "import nibabel as nib\n",
    "\n",
    "file = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/subj{subject}/func1pt8mm/roi/nsdgeneral.nii.gz\"\n",
    "nifti = nib.load(file)\n",
    "nsd_mask = nifti.get_fdata()\n",
    "nsd_mask = nsd_mask>0\n",
    "nsd_mask = torch.from_numpy(nsd_mask)\n",
    "print(nsd_mask.shape)\n",
    "print(torch.sum(nsd_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f0c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get roi class for given roi\n",
    "def getRoiClass(roi):\n",
    "    if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "        roi_class = 'prf-visualrois'\n",
    "    elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "        roi_class = 'floc-bodies'\n",
    "    elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "        roi_class = 'floc-faces'\n",
    "    elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "        roi_class = 'floc-places'\n",
    "    elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "        roi_class = 'floc-words'\n",
    "    elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "        roi_class = 'streams'\n",
    "    elif roi in [\"nsdgeneral\"]:\n",
    "        roi_class = 'nsdgeneral'\n",
    "    else:\n",
    "        raise ValueError('invalid ROI')\n",
    "    return roi_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e70f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns nsdgeneral roi mask\n",
    "def getRoiMask(hemisphere, roi):\n",
    "    # get ROI class for given ROI\n",
    "    roi_class = getRoiClass(roi)\n",
    "    file_roi = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/subj{subject}/func1pt8mm/roi/{hemisphere[0]}h.{roi_class}.nii.gz\"\n",
    "    nifti_roi = nib.load(file_roi)\n",
    "    mask_roi = nifti_roi.get_fdata()\n",
    "    \n",
    "    # get integer indices for ROI's mask in each ROI class\n",
    "    file_map = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/freesurfer/subj{subject}/label/{roi_class}.mgz.ctab\"\n",
    "    with open(file_map, \"r\") as file1:\n",
    "        map_list = file1.readlines()\n",
    "        map_dict = {}\n",
    "        for label in map_list:\n",
    "            roi_label = label.split()\n",
    "            map_dict.update({roi_label[1]: int(roi_label[0])})\n",
    "\n",
    "    label_int = map_dict[roi]\n",
    "    mask_roi[mask_roi!=label_int] = 0\n",
    "    mask_roi = np.asarray(mask_roi,bool)\n",
    "    mask_roi = mask_roi[nsd_mask==1]\n",
    "    \n",
    "    return mask_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47e832",
   "metadata": {},
   "source": [
    "### Run through all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d43c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dw26/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [04:35, 41.24s/it]"
     ]
    }
   ],
   "source": [
    "feature_count = 13\n",
    "# get correlation by ROI\n",
    "roi_list = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"nsdgeneral\"]\n",
    "\n",
    "feature_correlations_lh = np.zeros((feature_count, len(roi_list)))\n",
    "feature_correlations_rh = np.zeros((feature_count, len(roi_list)))\n",
    "\n",
    "for feature_num in range(feature_count):\n",
    "    ### load pretrained alexnet\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet') # run in commandline if not loading\n",
    "    model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "    model.eval() # set the model to evaluation mode, since you are not training it\n",
    "    # load model\n",
    "    model_layer = \"features.\" + str(feature_num)  # choose layer\n",
    "    print(model_layer)\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])\n",
    "\n",
    "    pca = fit_pca(feature_extractor, train_dl)\n",
    "\n",
    "    features_train, fmri_train = extract_features(feature_extractor, train_dl, pca)\n",
    "    features_val, fmri_val = extract_features(feature_extractor, val_dl, pca)\n",
    "\n",
    "    del model, pca\n",
    "\n",
    "### Linearly map downsampled alexnet features to fMRI\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Fit linear regressions on the training data\n",
    "    reg = LinearRegression().fit(features_train, fmri_train)\n",
    "    # Use fitted linear regressions to predict the validation and test fMRI data\n",
    "    fmri_val_pred = reg.predict(features_val)\n",
    "\n",
    "# Evaluations\n",
    "\n",
    "    from scipy.stats import pearsonr as corr\n",
    "\n",
    "    correlation = np.zeros(fmri_val_pred.shape[1])\n",
    "    # Correlate each predicted voxel with the corresponding ground truth voxel\n",
    "    for v in tqdm(range(fmri_val_pred.shape[1])):\n",
    "        correlation[v] = corr(fmri_val_pred[:,v], fmri_val[:,v])[0]\n",
    "   \n",
    "    lh_roi_correlation = []\n",
    "    rh_roi_correlation = []\n",
    "    for hemisphere in ['left', 'right']:\n",
    "        for roi in tqdm(roi_list):\n",
    "            mask_roi = getRoiMask(hemisphere,roi)\n",
    "            correlation_roi = correlation[mask_roi==1]\n",
    "\n",
    "            if hemisphere == 'left':\n",
    "                lh_roi_correlation.append(correlation_roi)\n",
    "            else:\n",
    "                rh_roi_correlation.append(correlation_roi)\n",
    "            \n",
    "\n",
    "    feature_correlations_lh[feature_num, :] = [np.median(lh_roi_correlation[r]) for r in range(len(lh_roi_correlation))]\n",
    "    feature_correlations_rh[feature_num, :] = [np.median(rh_roi_correlation[r]) for r in range(len(rh_roi_correlation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42cd1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "x = np.arange(feature_count)\n",
    "\n",
    "width = 0.30\n",
    "# plt.plot(x - width/2, feature_correlations_lh, width, label=roi_list)\n",
    "lines = plt.plot(x, feature_correlations_rh, width,label=roi_list)\n",
    "print(len(lines))\n",
    "plt.xlim(left=min(x), right=max(x)+3.1)\n",
    "# plt.ylim(bottom=0, top=1)\n",
    "plt.xlabel('features')\n",
    "plt.xticks(ticks=x, rotation=60)\n",
    "plt.ylabel('Median Pearson\\'s $r$')\n",
    "plt.legend(handles = lines[:len(roi_list)],frameon=True, loc='lower right', ncol = 2);\n",
    "plt.title(\"model_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get best feature layer for each ROI by getting index of best row entry in each column of correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd678bf",
   "metadata": {},
   "source": [
    "# Map from native voxel space to fsaverage surface space\n",
    "The Algonauts competition requires results in fsaverage space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f2b43",
   "metadata": {},
   "source": [
    "## Save voxel predictions as 3d Nifti file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7b927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn.image import load_img, new_img_like\n",
    "# temp_s3_dir = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3'\n",
    "temp_s3_dir = '/scratch/gpfs/KNORMAN/natural-scenes-dataset'\n",
    "\n",
    "roi = roi_list[7]\n",
    "print(roi)\n",
    "\n",
    "mask_roi = getRoiMask('left',roi)\n",
    "print(mask_roi.shape)\n",
    "print(np.sum(mask_roi.astype(int)))\n",
    "# data_to_convert = np.copy(fmri_val_pred[0]) # example activation\n",
    "data_to_convert = np.copy(correlation) # correlations\n",
    "\n",
    "data_to_convert[mask_roi==False]=0\n",
    "mask = torch.clone(nsd_mask).numpy()\n",
    "mask[mask==True]=data_to_convert\n",
    "data_to_convert = mask\n",
    "data_to_convert = np.array(data_to_convert).astype(np.float32)\n",
    "# data_to_convert = np.moveaxis(data_to_convert,0,-1)\n",
    "\n",
    "out_niimg_path = 'out.nii.gz'\n",
    "avg_out_niimg_path = 'avg_out.nii.gz'\n",
    "\n",
    "ref_niimg = load_img(f'{temp_s3_dir}/nsddata_betas/ppdata/subj{subject}/func1pt8mm/betas_fithrf_GLMdenoise_RR/R2_session01.nii.gz')\n",
    "assert ref_niimg.get_fdata().shape == data_to_convert.shape[:3]\n",
    "\n",
    "out_niimg = new_img_like(ref_niimg, data_to_convert, affine=None, copy_header=True)\n",
    "out_niimg.to_filename(out_niimg_path)\n",
    "print(out_niimg.shape)\n",
    "\n",
    "# also output a nifti volume averaging across the first dimension\n",
    "# avg_out_niimg = new_img_like(ref_niimg, np.mean(data_to_convert,axis=3), affine=None, copy_header=True)\n",
    "avg_out_niimg = new_img_like(ref_niimg, np.mean(data_to_convert,axis=0), affine=None, copy_header=True)\n",
    "avg_out_niimg.to_filename(avg_out_niimg_path)\n",
    "print(avg_out_niimg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a752b4-b8f8-48af-8d9d-8a34faad7377",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize brain volume (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb82a12-b860-410d-82e7-acaa277545fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "anat = load_img(f'{temp_s3_dir}/nsddata/ppdata/subj01/anat/T1_0pt8_masked.nii.gz')\n",
    "# plotting.plot_anat(anat).add_overlay(avg_out_niimg,cmap=plotting.cm.cold_hot,clim=[-3,3])\n",
    "plotting.plot_anat(anat).add_overlay(out_niimg,cmap=plotting.cm.cold_hot,clim=[-3,3])\n",
    "# plotting.view_img(avg_out_niimg,bg_img=anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f5f6c",
   "metadata": {},
   "source": [
    "## Convert Nifti from native voxel space -> native surface space -> fsaverage surface space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b3626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nsdcode.nsd_mapdata import NSDmapdata\n",
    "from nsdcode.nsd_datalocation import nsd_datalocation\n",
    "nsd = NSDmapdata(temp_s3_dir)\n",
    "\n",
    "# Example terminal commands to cp NSD buckets temporarily to local storage \n",
    "# remove \"--recursive\" if copying a single file instead of contents of a folder\n",
    "# in_dir = 'nsddata/ppdata/subj01/transforms/lh.MNI-to-layerB3.mgz'\n",
    "# print(f\"aws s3 cp --recursive s3://natural-scenes-dataset/{in_dir} {temp_s3_dir}/{in_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b969c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to subject-specific surface space across the three cortical depths\n",
    "for hemisphere in ['lh','rh']:\n",
    "    data = []\n",
    "    for p in range(3):\n",
    "        data.append(\n",
    "            nsd.fit(\n",
    "                int(subject),\n",
    "                'func1pt8', # source space\n",
    "                f'{hemisphere}.layerB{p+1}', # target space\n",
    "                out_niimg_path,\n",
    "                'cubic',\n",
    "                badval=0\n",
    "            )\n",
    "        )\n",
    "    data = np.array(data)\n",
    "    assert np.all(data==0) == False\n",
    "\n",
    "    # Now we average results across the three cortical depths and use\n",
    "    # nearest-neighbor interpolation to bring the result to fsaverage.\n",
    "    surface_data = nsd.fit(\n",
    "        int(subject),\n",
    "        f'{hemisphere}.white',\n",
    "        'fsaverage',\n",
    "        np.mean(data, axis=0),\n",
    "        interptype=None,\n",
    "        badval=0,\n",
    "        fsdir=f'{temp_s3_dir}/nsddata/freesurfer/subj01')\n",
    "    if hemisphere=='lh':\n",
    "        lh_surface_data = surface_data\n",
    "    elif hemisphere=='rh':\n",
    "        rh_surface_data = surface_data\n",
    "    print(f\"hemisphere {hemisphere}: surface_data.shape=\", surface_data.shape)\n",
    "    \n",
    "lh_surface_data = lh_surface_data.astype(np.float32)\n",
    "rh_surface_data = rh_surface_data.astype(np.float32)\n",
    "\n",
    "assert not np.all(lh_surface_data == rh_surface_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551588c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8553c",
   "metadata": {},
   "source": [
    "## Visualize surface activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7ef8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the interactive brain surface map\n",
    "from nilearn import datasets\n",
    "for hemisphere in ['left','right']:\n",
    "    if hemisphere=='left':\n",
    "        surface_data = lh_surface_data\n",
    "    elif hemisphere=='right':\n",
    "        surface_data = rh_surface_data\n",
    "    # fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "    fsaverage = datasets.fetch_surf_fsaverage('fsaverage', data_dir='/scratch/gpfs/dw26')\n",
    "    print(surface_data.shape)\n",
    "    plotting.plot_surf(\n",
    "        surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "#         surf_map=np.mean(surface_data,axis=1),\n",
    "        surf_map = surface_data,\n",
    "        bg_map=fsaverage['sulc_'+hemisphere],\n",
    "        hemi=hemisphere,\n",
    "        view='lateral',\n",
    "        threshold=1e-14,\n",
    "        cmap='cold_hot',\n",
    "        colorbar=True,\n",
    "        title='All vertices, '+hemisphere+' hemisphere'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40251134",
   "metadata": {},
   "source": [
    "# Save results in a way compatible with Algonauts submission\n",
    "After predicted data of all subjects and hemispheres have been saved, zip the parent submission folder (algonauts_2023_challenge_submission), which can then be submitted to CodaLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db81bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f'algonauts_2023_challenge_submission/subj{subject}/lh_pred_test.npy',lh_surface_data)\n",
    "np.save(f'algonauts_2023_challenge_submission/subj{subject}/rh_pred_test.npy',rh_surface_data)\n",
    "print(\"Saved predictions!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
