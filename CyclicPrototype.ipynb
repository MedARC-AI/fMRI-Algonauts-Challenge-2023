{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5992b5",
   "metadata": {},
   "source": [
    "# Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e67ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# get Clipper\n",
    "sys.path.append('../fMRI-reconstruction-NSD/src')\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7105b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404d060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../algonauts_2023_challenge_data'\n",
    "parent_submission_dir = 'algonauts_2023_challenge_submission'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "subj = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48530f",
   "metadata": {},
   "source": [
    "# Load Algonauts Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "441fe22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d9c7b",
   "metadata": {},
   "source": [
    "### Get Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c99a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    }
   ],
   "source": [
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8505aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "rand_seed = 5 #@param\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce5dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: are these transforms right for CLIP?\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)), # resize the images to 224x24 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab662d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img).to(device)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc41e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300 #@param\n",
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_val, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test, transform), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d4a8d",
   "metadata": {},
   "source": [
    "# Image to CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab91dfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n"
     ]
    }
   ],
   "source": [
    "# get clipper\n",
    "clip_extractor = models.Clipper(\"ViT-L/14\", device=torch.device(device)) # run in terminal to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87bc5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test clipper\n",
    "test_image_batch = next(iter(train_imgs_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d810aa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 3, 256, 256])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(test_image_batch.shape)\n",
    "print(test_image_batch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc448cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 3, 256, 256])\n",
      "cuda:0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(test_image_batch.shape)\n",
    "print(test_image_batch.device)\n",
    "print(clip_extractor.device)\n",
    "test_clip = clip_extractor.embed_image(test_image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e82de3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 768])\n"
     ]
    }
   ],
   "source": [
    "print(test_clip.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880d109",
   "metadata": {},
   "source": [
    "### CLIP Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad1535",
   "metadata": {},
   "source": [
    "# Clip2Voxel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0922d9c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da886e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e1ca689",
   "metadata": {},
   "source": [
    "### Get fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd55b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images Ã— LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images Ã— RH vertices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del lh_fmri_train, lh_fmri_val,rh_fmri_train,rh_fmri_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe58aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
