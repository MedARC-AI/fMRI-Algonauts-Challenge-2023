{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22895cd8",
   "metadata": {},
   "source": [
    "# Import packages + functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712b213c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "from subprocess import call\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc04fa1",
   "metadata": {},
   "source": [
    "# Load NSD webdatasets for a given subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6ef00",
   "metadata": {},
   "source": [
    "The '/fsx' directory refers to a directory housed on the Stability HPC. You will need to download NSD data from [our huggingface dataset](https://huggingface.co/datasets/pscotti/naturalscenesdataset/tree/main) and change the folders below to your own folder path. If you have demonstrated serious involvement with the project and need access to the Stability HPC contact Paul Scotti on Discord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbc9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # alexnet requirement\n",
    "])\n",
    "\n",
    "def preprocess(sample):\n",
    "    voxel, image = sample\n",
    "    return voxel, transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67101d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_devices 1\n",
      "num_workers 1\n",
      "batch_size 500\n",
      "global_batch_size 500\n",
      "num_worker_batches 1\n",
      "validation: num_worker_batches 1\n"
     ]
    }
   ],
   "source": [
    "subject = '01'\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: # not using gpu, assuming you want to use the limited dataset\n",
    "    num_devices = 1\n",
    "    num_workers = 1\n",
    "    batch_size = 500\n",
    "    num_samples = 500 \n",
    "else: # using cuda, assuming you want to use the full dataset\n",
    "    num_workers = num_devices\n",
    "    batch_size = 300\n",
    "    num_samples = 24983 # see metadata.json\n",
    "print(\"num_devices\",num_devices)\n",
    "print(\"num_workers\",num_workers)\n",
    "print(\"batch_size\",batch_size)\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"global_batch_size\",global_batch_size)\n",
    "num_batches = math.floor(num_samples / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "print(\"num_worker_batches\",num_worker_batches)\n",
    "# train_url = f'/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/train/train_subj{subject}'\n",
    "train_url = f'/scratch/gpfs/KNORMAN/webdataset_nsd/webdataset_split/train/train_subj{subject}'\n",
    "train_url = train_url+\"_{0..49}.tar\"\n",
    "\n",
    "train_data = wds.DataPipeline([wds.ResampledShards(train_url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    wds.shuffle(500,initial=500),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", 'images'),\n",
    "                    wds.map(preprocess),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=None, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Validation #\n",
    "num_samples = 492\n",
    "num_batches = math.ceil(num_samples / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "print(\"validation: num_worker_batches\",num_worker_batches)\n",
    "\n",
    "# val_url = f'/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/val/val_subj{subject}_0.tar'\n",
    "val_url = f'/scratch/gpfs/KNORMAN/webdataset_nsd/webdataset_split/val/val_subj{subject}_0.tar'\n",
    "val_data = wds.DataPipeline([wds.SimpleShardList(val_url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", 'images'),\n",
    "                    wds.map(preprocess),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7cbd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([500, 15724]) tensor([-0.3694, -0.5073, -1.4805, -1.3330], dtype=torch.float16)\n",
      "img_input.shape torch.Size([500, 3, 256, 256]) tensor([-0.8335, -0.8678, -0.8849, -0.8849])\n"
     ]
    }
   ],
   "source": [
    "# check that your data loaders are working\n",
    "for train_i, (voxel, img_input) in enumerate(train_dl):\n",
    "    print(\"idx\",train_i)\n",
    "    print(\"voxel.shape\",voxel.shape,voxel[0,20:24])\n",
    "    print(\"img_input.shape\",img_input.shape,img_input[0,0,0,100:104])\n",
    "    if train_i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40874f",
   "metadata": {},
   "source": [
    "# Train model (input = image; output = predicted fMRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a408b",
   "metadata": {},
   "source": [
    "### load pretrained alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728fd753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dw26/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet')\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6cc76",
   "metadata": {},
   "source": [
    "### Extract and downsample chosen alexnet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50470ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_layer = \"features.2\" \n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b355bb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor):\n",
      "    features_0 = getattr(self.features, \"0\")(x);  x = None\n",
      "    features_1 = getattr(self.features, \"1\")(features_0);  features_0 = None\n",
      "    features_2 = getattr(self.features, \"2\")(features_1);  features_1 = None\n",
      "    return {'features.2': features_2}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72a0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental PCA computes features\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=100, batch_size=batch_size)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, (voxel, img) in tqdm(enumerate(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(img.to(device))\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b40875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:15, 15.54s/it]\n"
     ]
    }
   ],
   "source": [
    "pca = fit_pca(feature_extractor, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9fd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets principal components of features for each images \n",
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    for _, (voxel, img) in tqdm(enumerate(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(img)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22cae371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:09,  9.69s/it]\n",
      "1it [00:05,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training images features:\n",
      "(500, 100)\n",
      "(Training stimulus images × PCA features)\n",
      "\n",
      "Validation images features:\n",
      "(300, 100)\n",
      "(Validation stimulus images × PCA features)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features_train = extract_features(feature_extractor, train_dl, pca)\n",
    "features_val = extract_features(feature_extractor, val_dl, pca)\n",
    "\n",
    "print('\\nTraining images features:')\n",
    "print(features_train.shape)\n",
    "print('(Training stimulus images × PCA features)')\n",
    "\n",
    "print('\\nValidation images features:')\n",
    "print(features_val.shape)\n",
    "print('(Validation stimulus images × PCA features)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921fa33",
   "metadata": {},
   "source": [
    "### mask for nsdgeneral voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebde86c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 104, 83])\n",
      "tensor(15724)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "file = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/subj{subject}/func1pt8mm/roi/nsdgeneral.nii.gz\"\n",
    "nifti = nib.load(file)\n",
    "mask = nifti.get_fdata()\n",
    "mask[mask<1] = 0\n",
    "mask = np.asarray(mask,bool)\n",
    "mask = torch.from_numpy(mask)\n",
    "print(mask.shape)\n",
    "print(torch.sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba38499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For whole_brain to 1D\n",
    "# def to1Dnsdgeneral(voxels3D, mask):\n",
    "#     voxel1D = torch.flatten(voxels3D)\n",
    "#     mask1D = mask.flatten()\n",
    "#     return [voxel for vox_idx, voxel in enumerate(voxel) if mask1D[vox_idx]]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86b8d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel, _ = next(iter(train_dl))\n",
    "# print(voxel.shape)\n",
    "# voxel1D = torch.flatten(voxel)\n",
    "# print(voxel1D.shape)\n",
    "# mask1D = mask.flatten()\n",
    "# print(mask1D.shape)\n",
    "# print(to1Dnsdgeneral(voxel, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed1cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxels1D is 1D tensor of nsdgeneral voxels \n",
    "# mask is 3D tensor of 3D whole brain space\n",
    "def to3DwholeBrain(voxels1D, mask):\n",
    "    mask1D = torch.flatten(mask)\n",
    "    voxels3D = torch.zeros_like(mask1D)\n",
    "    index_1D = 0\n",
    "    for mask_idx, masking in enumerate(mask1D):\n",
    "        if masking:\n",
    "            voxels3D[mask_idx]=voxels1D[index_1D]\n",
    "            index_1D+=1\n",
    "        else:\n",
    "            voxels3D[mask_idx]=0\n",
    "    return torch.reshape(voxels3D,  mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa1d5e",
   "metadata": {},
   "source": [
    "### Linearly map downsampled alexnet features to fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e750f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create voxel matrix for whole brain\n",
    "# def getVoxels(dataloader):\n",
    "#     fmri = []\n",
    "#     for _, (voxel, img) in tqdm(enumerate(dataloader)):\n",
    "#         # TODO: first have to extract each batch dimension, then apply mask to each image activation, then append\n",
    "#         fmri.append(to1Dnsdgeneral(voxel, mask))\n",
    "#     return np.array(fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f6e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI training shape: torch.Size([500, 15724])\n",
      "fMRI validation shape: torch.Size([300, 15724])\n"
     ]
    }
   ],
   "source": [
    "# fmri_train = getVoxels(train_dl)\n",
    "# fmri_val = getVoxels(val_dl)\n",
    "# for batch size == num_samples\n",
    "fmri_train, _ = next(iter(train_dl))\n",
    "fmri_val, _ = next(iter(val_dl))\n",
    "\n",
    "print(\"fMRI training shape:\",fmri_train.shape)\n",
    "print(\"fMRI validation shape:\",fmri_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d8ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4658, -0.4373,  3.4473,  ..., -0.0536,  0.0643, -0.4294],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(fmri_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "192a7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit linear regressions on the training data\n",
    "reg = LinearRegression().fit(features_train, fmri_train)\n",
    "# Use fitted linear regressions to predict the validation and test fMRI data\n",
    "fmri_val_pred = reg.predict(features_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f74c2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 15724)\n",
      "torch.Size([300, 15724])\n",
      "torch.Size([81, 104, 83])\n"
     ]
    }
   ],
   "source": [
    "print(fmri_val_pred.shape)\n",
    "print(fmri_val.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d165fe",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1432ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 15724/15724 [00:01<00:00, 8930.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr as corr\n",
    "\n",
    "correlation = np.zeros(fmri_val_pred.shape[1])\n",
    "# Correlate each predicted voxel with the corresponding ground truth voxel\n",
    "for v in tqdm(range(fmri_val_pred.shape[1])):\n",
    "    correlation[v] = corr(fmri_val_pred[:,v], fmri_val[:,v])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9729c4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15724,)\n"
     ]
    }
   ],
   "source": [
    "print(correlation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90bfd644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRoiClass(roi):\n",
    "    if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "            roi_class = 'prf-visualrois'\n",
    "    elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "        roi_class = 'floc-bodies'\n",
    "    elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "        roi_class = 'floc-faces'\n",
    "    elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "        roi_class = 'floc-places'\n",
    "    elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "        roi_class = 'floc-words'\n",
    "    elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "        roi_class = 'streams'\n",
    "    return roi_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 31/31 [05:17<00:00, 10.23s/it]\n",
      " 16%|█████████▌                                                 | 5/31 [00:51<04:30, 10.39s/it]"
     ]
    }
   ],
   "source": [
    "# Mask for ROI\n",
    "hemisphere = 'left' # 'left' or 'right'\n",
    "roi_list = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
    "# roi = roi_list[7]\n",
    "# print(roi)\n",
    "\n",
    "lh_roi_correlation = []\n",
    "rh_roi_correlation = []\n",
    "for hemisphere in ['left', 'right']:\n",
    "    for roi in tqdm(roi_list):\n",
    "        roi_class = getRoiClass(roi)\n",
    "\n",
    "        file_roi = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/subj{subject}/func1pt8mm/roi/{hemisphere[0]}h.{roi_class}.nii.gz\"\n",
    "        nifti_roi = nib.load(file_roi)\n",
    "        mask_roi = nifti_roi.get_fdata()\n",
    "\n",
    "        file_map = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/freesurfer/subj{subject}/label/{roi_class}.mgz.ctab\"\n",
    "        with open(file_map, \"r\") as file1:\n",
    "            map_list = file1.readlines()\n",
    "            map_dict = {}\n",
    "            for label in map_list:\n",
    "                roi_label = label.split()\n",
    "                map_dict.update({roi_label[1]: int(roi_label[0])})\n",
    "\n",
    "        label_int = map_dict[roi]\n",
    "\n",
    "        mask_roi[mask_roi!=label_int] = 0\n",
    "        mask_roi = np.asarray(mask_roi,bool)\n",
    "        mask_roi = torch.from_numpy(mask_roi)\n",
    "    \n",
    "        voxels_corr = to3DwholeBrain(correlation, mask_roi)\n",
    "        voxels_corr = voxels_corr.flatten()\n",
    "        voxels_corr = [v for v in voxels_corr if v!=0]\n",
    "        \n",
    "                    \n",
    "        if hemisphere == 'left':\n",
    "            lh_roi_correlation.append(voxels_corr)\n",
    "        else:\n",
    "            rh_roi_correlation.append(voxels_corr)\n",
    "\n",
    "\n",
    "\n",
    "# print(mask_roi.shape)\n",
    "# print(torch.sum(mask_roi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "lh_median_roi_correlation = [np.median(lh_roi_correlation[r]) for r in range(len(lh_roi_correlation))]\n",
    "rh_median_roi_correlation = [np.median(rh_roi_correlation[r])\n",
    "    for r in range(len(rh_roi_correlation))]\n",
    "plt.figure(figsize=(18,6))\n",
    "x = np.arange(len(roi_list))\n",
    "width = 0.30\n",
    "plt.bar(x - width/2, lh_median_roi_correlation, width, label='Left Hemisphere')\n",
    "plt.bar(x + width/2, rh_median_roi_correlation, width,\n",
    "    label='Right Hemishpere')\n",
    "plt.xlim(left=min(x)-.5, right=max(x)+.5)\n",
    "plt.ylim(bottom=0, top=1)\n",
    "plt.xlabel('ROIs')\n",
    "plt.xticks(ticks=x, labels=roi_list, rotation=60)\n",
    "plt.ylabel('Median Pearson\\'s $r$')\n",
    "plt.legend(frameon=True, loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd678bf",
   "metadata": {},
   "source": [
    "# Map from native voxel space to fsaverage surface space\n",
    "The Algonauts competition requires results in fsaverage space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f2b43",
   "metadata": {},
   "source": [
    "## Save voxel predictions as 3d Nifti file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7b927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn.image import load_img, new_img_like\n",
    "# temp_s3_dir = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3'\n",
    "temp_s3_dir = '/scratch/gpfs/KNORMAN/natural-scenes-dataset'\n",
    "\n",
    "print(mask_roi.shape)\n",
    "data_to_convert = to3DwholeBrain(fmri_val_pred[0], mask_roi)\n",
    "print(data_to_convert.shape)\n",
    "data_to_convert = np.array(data_to_convert).astype(np.float32)\n",
    "# data_to_convert = np.moveaxis(data_to_convert,0,-1)\n",
    "\n",
    "out_niimg_path = 'out.nii.gz'\n",
    "avg_out_niimg_path = 'avg_out.nii.gz'\n",
    "\n",
    "ref_niimg = load_img(f'{temp_s3_dir}/nsddata_betas/ppdata/subj{subject}/func1pt8mm/betas_fithrf_GLMdenoise_RR/R2_session01.nii.gz')\n",
    "print(ref_niimg.get_fdata().shape)\n",
    "print(data_to_convert.shape)\n",
    "assert ref_niimg.get_fdata().shape == data_to_convert.shape[:3]\n",
    "\n",
    "out_niimg = new_img_like(ref_niimg, data_to_convert, affine=None, copy_header=True)\n",
    "out_niimg.to_filename(out_niimg_path)\n",
    "print(out_niimg.shape)\n",
    "\n",
    "# also output a nifti volume averaging across the first dimension\n",
    "# avg_out_niimg = new_img_like(ref_niimg, np.mean(data_to_convert,axis=3), affine=None, copy_header=True)\n",
    "avg_out_niimg = new_img_like(ref_niimg, np.mean(data_to_convert,axis=0), affine=None, copy_header=True)\n",
    "avg_out_niimg.to_filename(avg_out_niimg_path)\n",
    "print(avg_out_niimg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a752b4-b8f8-48af-8d9d-8a34faad7377",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize brain volume (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb82a12-b860-410d-82e7-acaa277545fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "anat = load_img(f'{temp_s3_dir}/nsddata/ppdata/subj01/anat/T1_0pt8_masked.nii.gz')\n",
    "# plotting.plot_anat(anat).add_overlay(avg_out_niimg,cmap=plotting.cm.cold_hot,clim=[-3,3])\n",
    "plotting.plot_anat(anat).add_overlay(out_niimg,cmap=plotting.cm.cold_hot,clim=[-3,3])\n",
    "# plotting.view_img(avg_out_niimg,bg_img=anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f5f6c",
   "metadata": {},
   "source": [
    "## Convert Nifti from native voxel space -> native surface space -> fsaverage surface space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b3626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nsdcode.nsd_mapdata import NSDmapdata\n",
    "from nsdcode.nsd_datalocation import nsd_datalocation\n",
    "nsd = NSDmapdata(temp_s3_dir)\n",
    "\n",
    "# Example terminal commands to cp NSD buckets temporarily to local storage \n",
    "# remove \"--recursive\" if copying a single file instead of contents of a folder\n",
    "# in_dir = 'nsddata/ppdata/subj01/transforms/lh.MNI-to-layerB3.mgz'\n",
    "# print(f\"aws s3 cp --recursive s3://natural-scenes-dataset/{in_dir} {temp_s3_dir}/{in_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b969c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to subject-specific surface space across the three cortical depths\n",
    "for hemisphere in ['lh','rh']:\n",
    "    data = []\n",
    "    for p in range(3):\n",
    "        data.append(\n",
    "            nsd.fit(\n",
    "                int(subject),\n",
    "                'func1pt8', # source space\n",
    "                f'{hemisphere}.layerB{p+1}', # target space\n",
    "                out_niimg_path,\n",
    "                'cubic',\n",
    "                badval=0\n",
    "            )\n",
    "        )\n",
    "    data = np.array(data)\n",
    "    assert np.all(data==0) == False\n",
    "\n",
    "    # Now we average results across the three cortical depths and use\n",
    "    # nearest-neighbor interpolation to bring the result to fsaverage.\n",
    "    surface_data = nsd.fit(\n",
    "        int(subject),\n",
    "        f'{hemisphere}.white',\n",
    "        'fsaverage',\n",
    "        np.mean(data, axis=0),\n",
    "        interptype=None,\n",
    "        badval=0,\n",
    "        fsdir=f'{temp_s3_dir}/nsddata/freesurfer/subj01')\n",
    "    if hemisphere=='lh':\n",
    "        lh_surface_data = surface_data\n",
    "    elif hemisphere=='rh':\n",
    "        rh_surface_data = surface_data\n",
    "    print(f\"hemisphere {hemisphere}: surface_data.shape=\", surface_data.shape)\n",
    "    \n",
    "lh_surface_data = lh_surface_data.astype(np.float32)\n",
    "rh_surface_data = rh_surface_data.astype(np.float32)\n",
    "\n",
    "assert not np.all(lh_surface_data == rh_surface_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8553c",
   "metadata": {},
   "source": [
    "## Visualize surface activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7ef8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the interactive brain surface map\n",
    "from nilearn import datasets\n",
    "for hemisphere in ['left','right']:\n",
    "    if hemisphere=='left':\n",
    "        surface_data = lh_surface_data\n",
    "    elif hemisphere=='right':\n",
    "        surface_data = rh_surface_data\n",
    "    # fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "    fsaverage = datasets.fetch_surf_fsaverage('fsaverage', data_dir='/scratch/gpfs/dw26')\n",
    "    print(surface_data.shape)\n",
    "    plotting.plot_surf(\n",
    "        surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "#         surf_map=np.mean(surface_data,axis=1),\n",
    "        surf_map = surface_data,\n",
    "        bg_map=fsaverage['sulc_'+hemisphere],\n",
    "        hemi=hemisphere,\n",
    "        view='lateral',\n",
    "        threshold=1e-14,\n",
    "        cmap='cold_hot',\n",
    "        colorbar=True,\n",
    "        title='All vertices, '+hemisphere+' hemisphere'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40251134",
   "metadata": {},
   "source": [
    "# Save results in a way compatible with Algonauts submission\n",
    "After predicted data of all subjects and hemispheres have been saved, zip the parent submission folder (algonauts_2023_challenge_submission), which can then be submitted to CodaLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db81bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f'algonauts_2023_challenge_submission/subj{subject}/lh_pred_test.npy',lh_surface_data)\n",
    "np.save(f'algonauts_2023_challenge_submission/subj{subject}/rh_pred_test.npy',rh_surface_data)\n",
    "print(\"Saved predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf488c-8458-4b49-83c9-9469825f7037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
