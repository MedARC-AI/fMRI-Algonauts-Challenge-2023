{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get custom models and functions\n",
    "import sys\n",
    "# sys.path.append('../fMRI-reconstruction-NSD/src')\n",
    "sys.path.append('/fsx/proj-medarc/fmri/fMRI-reconstruction-NSD/src/')\n",
    "from models import Clipper, BrainNetwork\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm # for jupyter notebook\n",
    "from accelerate import Accelerator\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "# import Iprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '../algonauts_2023_challenge_data'\n",
    "data_dir = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/dataset\"\n",
    "# parent_submission_dir = 'algonauts_2023_challenge_submission'\n",
    "parent_submission_dir = \"/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/algonauts_2023_challenge_submission\"\n",
    "\n",
    "subj = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=testing', '--clip_variant=ViT-L/14', '--batch_size=128']\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# can specify jupyter_args here for argparser to use if running this code interactively\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=testing\")\n",
    "    jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    jupyter_args.append(\"--batch_size=128\") # smaller to account for more loaded models.\n",
    "#     jupyter_args.append(\"--resume_from_ckpt\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if not using wandb and want to resume from a ckpt\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--resume_from_ckpt',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if not using wandb and want to resume from a ckpt.',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--no_ckpt_saving',\n",
    "    action='store_false',\n",
    "    default=True,\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=1,\n",
    "    help=\"save ckpt every x epochs\",\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--save_at_end',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if False, will save best.ckpt whenever epoch shows best validation score',\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--seed\",type=int,default=42,\n",
    "# )\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Models and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for dataloaders\n",
    "\n",
    "def _transforms(n_px=256, reshape=False):\n",
    "    if reshape:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.CenterCrop(n_px),\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    return transform\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            n_px = min(img.size)\n",
    "            img = self.transform(n_px, reshape=True)(img).to(device)\n",
    "        else:\n",
    "            img = _transforms()(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training COCO images: 111063\n",
      "\n",
      "Validation COCO images: 12340\n"
     ]
    }
   ],
   "source": [
    "# coco_dir = \"/scratch/gpfs/KNORMAN/COCO/unlabeled2017_all\"\n",
    "coco_dir = \"/fsx/proj-medarc/fmri/coco/unlabeled2017\"\n",
    "\n",
    "# Create list with all file names, sorted\n",
    "coco_list = os.listdir(coco_dir)\n",
    "coco_list.sort()\n",
    "\n",
    "rand_seed = 5 \n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Calculate how many COCO images correspond to 90% for training\n",
    "num_train_coco = int(np.round(len(coco_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs_coco = np.arange(len(coco_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train_coco, idxs_val_coco = idxs_coco[:num_train_coco], idxs_coco[num_train_coco:]\n",
    "\n",
    "print('Training COCO images: ' + format(len(idxs_train_coco)))\n",
    "print('\\nValidation COCO images: ' + format(len(idxs_val_coco)))\n",
    "\n",
    "# Get the paths of all image files\n",
    "coco_paths = sorted(list(Path(coco_dir).iterdir()))\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_train_coco, _transforms), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_val_coco, _transforms), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Algonauts Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Get Algonauts fMRI data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')\n",
    "\n",
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]\n",
    "\n",
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algonauts Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, lh_fmri, rh_fmri, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "        self.lh_fmri = lh_fmri\n",
    "        self.rh_fmri = rh_fmri\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            n_px = min(img.size)\n",
    "            img = self.transform(n_px, reshape=True)(img).to(device)\n",
    "        else:\n",
    "            img = _transforms()(img)\n",
    "        return img, torch.from_numpy(self.lh_fmri[idx]), torch.from_numpy(self.rh_fmri[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# Algonauts dataloaders\n",
    "train_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_train, lh_fmri_train, rh_fmri_train), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_val, lh_fmri_val, rh_fmri_val), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "# just images\n",
    "test_alg_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# training_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/training_clip\"\n",
    "# validation_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/validation_clip\"\n",
    "# test_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/test_split/test_clip\"\n",
    "\n",
    "# if not os.path.isdir(training_clip_dir):\n",
    "#     os.makedirs(training_clip_dir)\n",
    "# if not os.path.isdir(validation_clip_dir):\n",
    "#     os.makedirs(validation_clip_dir)\n",
    "# if not os.path.isdir(test_clip_dir):\n",
    "#     os.makedirs(test_clip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cpu\n"
     ]
    }
   ],
   "source": [
    "# get clipper\n",
    "clip_extractor = Clipper(\"ViT-L/14\", device=torch.device(device)) # run in terminal to download\n",
    "\n",
    "# def img2clipfile(img_paths, dataloader, idxs, clip_dir):\n",
    "#     running_count=0\n",
    "#     for img_batch in tqdm(dataloader):\n",
    "#         clip_batch = clip_extractor.embed_image(img_batch)\n",
    "#         for clip in clip_batch:\n",
    "#             clip = clip.cpu().numpy()\n",
    "#             img_path = img_paths[idxs[running_count]]\n",
    "#             clip_file_name = str(img_path)[-24:-3]+\"npy\"\n",
    "#             np.save(os.path.join(training_clip_dir, clip_file_name), clip)\n",
    "#             running_count+=1\n",
    "        \n",
    "        \n",
    "# img2clipfile(train_imgs_paths, train_imgs_dataloader, idxs_train, training_clip_dir)\n",
    "# img2clipfile(train_imgs_paths, val_imgs_dataloader, idxs_val, validation_clip_dir)\n",
    "# img2clipfile(test_imgs_paths, test_imgs_dataloader, idxs_test, test_clip_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Clip2Vert and Vert2CLip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclePrototype(nn.Module):\n",
    "    def __init__(self, brain_dim, clip_dim, vert2clipPath=None):\n",
    "        super().__init__()\n",
    "        self.brain_dim = dict(out_dim=brain_dim, in_dim=clip_dim)\n",
    "        self.clip_dim = dict(out_dim=clip_dim, in_dim=brain_dim)\n",
    "        self.clip2vert = BrainNetwork(**self.brain_dim)\n",
    "        self.vert2clip = BrainNetwork(**self.clip_dim)\n",
    "\n",
    "        # loading pretrained vert2clip model\n",
    "        if vert2clipPath is not None:\n",
    "            self.vert2clipPath = vert2clipPath\n",
    "            checkpoint = torch.load(self.vert2clipPath, map_location=device)\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            self.vert2clip.load_state_dict(state_dict)\n",
    "            for param in self.vert2clip.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, paired=False):\n",
    "        brain = self.clip2vert(x)\n",
    "        x = self.vert2clip(brain)\n",
    "        # whether want both brain activations and clip embedding\n",
    "        if paired:\n",
    "            return brain, x\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain dimension: 39548\n",
      "CLIP dimension: 768\n",
      "params of cycle:\n",
      "param counts:\n",
      "464,649,596 total\n",
      "232,344,188 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "brain_dim = lh_fmri_train.shape[1]+rh_fmri_train.shape[1]\n",
    "clip_dim = 768\n",
    "model_path = \"/fsx/proj-medarc/fmri/ckadirt/github/fMRI-reconstruction-NSD/src/test/vert2clip/vert2clip-test_latest.pt\"\n",
    "cycle_kwargs = dict(brain_dim=brain_dim, clip_dim=clip_dim, vert2clipPath=model_path)\n",
    "cycle = CyclePrototype(**cycle_kwargs)\n",
    "# cycle = CyclePrototype(brain_dim=brain_dim, clip_dim=clip_dim, vert2clipPath=model_path)\n",
    "print(\"Brain dimension:\", brain_dim)\n",
    "print(\"CLIP dimension:\", clip_dim)\n",
    "\n",
    "print(\"params of cycle:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(cycle)\n",
    "    \n",
    "no_decay = ['bias']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in cycle.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in cycle.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=3e-4) # lr doesnt get used if lr_scheduler='cycle'\n",
    "\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    state_dict = cycle.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': cycle.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'lrs': lrs,\n",
    "        }, ckpt_path)\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Huggingface Accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algonauts images\n",
    "cycle, optimizer, train_alg_dl, val_alg_dl, test_alg_dl, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_alg_dataloader, val_alg_dataloader, test_alg_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# COCO images\n",
    "cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing starting with epoch 0 / 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 0/120 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "training coco:   1%|          | 7/868 [06:30<13:20:11, 55.76s/it]\n",
      "epochs:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 0/120 [06:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     image \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m clip_target \u001b[39m=\u001b[39m clip_extractor\u001b[39m.\u001b[39;49membed_image(image)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# determines what kind of loss \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mif\u001b[39;00m j\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/fsx/proj-medarc/fmri/fMRI-reconstruction-NSD/src/models.py:170\u001b[0m, in \u001b[0;36mClipper.embed_image\u001b[0;34m(self, image, apply_transforms, apply_spatial_transforms)\u001b[0m\n\u001b[1;32m    167\u001b[0m     clip_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversatile_process_embeddings(clip_emb)\n\u001b[1;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m clip_emb\n\u001b[0;32m--> 170\u001b[0m clip_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip\u001b[39m.\u001b[39;49mencode_image(clip_emb)\n\u001b[1;32m    171\u001b[0m \u001b[39m# input is now in CLIP space, but mind-reader preprint further processes embeddings:\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclamp_embs:\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:337\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image\u001b[39m.\u001b[39;49mtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:228\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    225\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    227\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    229\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    231\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post(x[:, \u001b[39m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:199\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:186\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 186\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x))\n\u001b[1;32m    187\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x))\n\u001b[1;32m    188\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:183\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattention\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x, x, x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_mask)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/functional.py:5116\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5113\u001b[0m     v \u001b[39m=\u001b[39m static_v\n\u001b[1;32m   5115\u001b[0m \u001b[39m# add zero attention along batch dimension (now first)\u001b[39;00m\n\u001b[0;32m-> 5116\u001b[0m \u001b[39mif\u001b[39;00m add_zero_attn:\n\u001b[1;32m   5117\u001b[0m     zero_attn_shape \u001b[39m=\u001b[39m (bsz \u001b[39m*\u001b[39m num_heads, \u001b[39m1\u001b[39m, head_dim)\n\u001b[1;32m   5118\u001b[0m     k \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([k, torch\u001b[39m.\u001b[39mzeros(zero_attn_shape, dtype\u001b[39m=\u001b[39mk\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mk\u001b[39m.\u001b[39mdevice)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth')\n",
    "    epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    cycle.load_state_dict(checkpoint['model_state_dict'])\n",
    "    losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    \n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0), desc='epochs')\n",
    "for epoch in progress_bar:\n",
    "    cycle.train()\n",
    "    cocoRate = 1 # int number of COCO batches to algonauts batches\n",
    "    alpha = .5 # fraction of total loss due to brain space loss for algonauts batches\n",
    "    alg_batches = enumerate(tqdm(train_alg_dl, desc='training algonauts'))\n",
    "    for train_i, coco_image in enumerate(tqdm(train_coco_dl, desc='training coco')):\n",
    "        if train_i%cocoRate==0:\n",
    "            alg_batch = next(alg_batches, None)\n",
    "            # if end of algonauts dataloader reached, start over for another epoch\n",
    "            if alg_batch is None:\n",
    "                alg_batches = enumerate(tqdm(train_alg_dl, desc='training algonauts'))\n",
    "                alg_batch = next(alg_batches)\n",
    "            img_batches = [coco_image, alg_batch]\n",
    "        else:\n",
    "            img_batches = [coco_image]\n",
    "\n",
    "\n",
    "        for j, data in enumerate(img_batches):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        #         repeat_index = train_i % 3\n",
    "            if j!=0:\n",
    "                alg_index, (image, lh_vert, rh_vert) = data\n",
    "            else:\n",
    "                image = data\n",
    "\n",
    "            image = image.float()\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "            # determines what kind of loss \n",
    "            if j!=0:\n",
    "                brain_target = torch.cat((lh_vert, rh_vert), 1)\n",
    "                brain_pred, clip_pred = cycle(clip_target, paired=True) \n",
    "                brain_loss = mse(brain_target, brain_pred)\n",
    "                clip_loss = mse(clip_target, clip_pred)\n",
    "                loss = (1-alpha)*clip_loss+alpha*brain_loss\n",
    "            else:\n",
    "                clip_pred = cycle(clip_target) \n",
    "                loss = mse(clip_target, clip_pred)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    cycle.eval()\n",
    "    if local_rank==0: # i think its possible to remove this if statement though with some revisions\n",
    "        for val_i, image in enumerate(tqdm(val_coco_dl, desc='validation')): \n",
    "            with torch.no_grad():\n",
    "                # repeat_index = val_i % 3\n",
    "\n",
    "                image = image.float()\n",
    "\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "                clip_cycle = cycle(clip_target) \n",
    "                val_loss =  mse(clip_target, clip_cycle)\n",
    "                utils.check_loss(val_loss)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        if (not save_at_end and not no_ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "        \n",
    "        # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            save_ckpt(f'last')\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"val/num_steps\": len(val_losses)}\n",
    "        \n",
    "        progress_bar.set_postfix(**logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '1 COCO batches(s) for every 1 algonauts batch')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(losses))\n",
    "print(len(val_losses))\n",
    "plt.plot(losses)\n",
    "plt.title(str(cocoRate)+\" COCO batches(s) for every 1 algonauts batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
