{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "# get custom models and functions\n",
    "import sys\n",
    "sys.path.append('../fMRI-reconstruction-NSD/src')\n",
    "from models import Clipper, BrainNetwork\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from accelerate import Accelerator\n",
    "import argparse\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../algonauts_2023_challenge_data'\n",
    "parent_submission_dir = 'algonauts_2023_challenge_submission'\n",
    "\n",
    "subj = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=testing', '--clip_variant=ViT-L/14', '--batch_size=128']\n"
     ]
    }
   ],
   "source": [
    "# can specify jupyter_args here for argparser to use if running this code interactively\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=testing\")\n",
    "    jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    jupyter_args.append(\"--batch_size=128\") # smaller to account for more loaded models.\n",
    "#     jupyter_args.append(\"--resume_from_ckpt\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if not using wandb and want to resume from a ckpt\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--resume_from_ckpt',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if not using wandb and want to resume from a ckpt.',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--no_ckpt_saving',\n",
    "    action='store_false',\n",
    "    default=True,\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=1,\n",
    "    help=\"save ckpt every x epochs\",\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--save_at_end',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if False, will save best.ckpt whenever epoch shows best validation score',\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--seed\",type=int,default=42,\n",
    "# )\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Models and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for dataloaders\n",
    "\n",
    "def _transforms(n_px=256, reshape=False):\n",
    "    if reshape:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.CenterCrop(n_px),\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    return transform\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            n_px = min(img.size)\n",
    "            img = self.transform(n_px, reshape=True)(img).to(device)\n",
    "        else:\n",
    "            img = _transforms()(img)\n",
    "        return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training COCO images: 111063\n",
      "\n",
      "Validation COCO images: 12340\n"
     ]
    }
   ],
   "source": [
    "coco_dir = \"/scratch/gpfs/KNORMAN/COCO/unlabeled2017_all\"\n",
    "\n",
    "# Create list with all file names, sorted\n",
    "coco_list = os.listdir(coco_dir)\n",
    "coco_list.sort()\n",
    "\n",
    "rand_seed = 5 \n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Calculate how many COCO images correspond to 90% for training\n",
    "num_train_coco = int(np.round(len(coco_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs_coco = np.arange(len(coco_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train_coco, idxs_val_coco = idxs_coco[:num_train_coco], idxs_coco[num_train_coco:]\n",
    "\n",
    "print('Training COCO images: ' + format(len(idxs_train_coco)))\n",
    "print('\\nValidation COCO images: ' + format(len(idxs_val_coco)))\n",
    "\n",
    "# Get the paths of all image files\n",
    "coco_paths = sorted(list(Path(coco_dir).iterdir()))\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_train_coco, _transforms), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_val_coco, _transforms), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Algonauts Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Get Algonauts fMRI data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')\n",
    "\n",
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]\n",
    "\n",
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algonauts Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, lh_fmri, rh_fmri, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "        self.lh_fmri = lh_fmri\n",
    "        self.rh_fmri = rh_fmri\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.from_numpy(self.lh_fmri[idx]), torch.from_numpy(self.rh_fmri[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# Algonauts dataloaders\n",
    "train_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_train, lh_fmri_train, rh_fmri_train), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_val, lh_fmri_val, rh_fmri_val), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "# just images\n",
    "test_alg_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# training_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/training_clip\"\n",
    "# validation_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/validation_clip\"\n",
    "# test_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/test_split/test_clip\"\n",
    "\n",
    "# if not os.path.isdir(training_clip_dir):\n",
    "#     os.makedirs(training_clip_dir)\n",
    "# if not os.path.isdir(validation_clip_dir):\n",
    "#     os.makedirs(validation_clip_dir)\n",
    "# if not os.path.isdir(test_clip_dir):\n",
    "#     os.makedirs(test_clip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n"
     ]
    }
   ],
   "source": [
    "# get clipper\n",
    "clip_extractor = Clipper(\"ViT-L/14\", device=torch.device(device)) # run in terminal to download\n",
    "\n",
    "# def img2clipfile(img_paths, dataloader, idxs, clip_dir):\n",
    "#     running_count=0\n",
    "#     for img_batch in tqdm(dataloader):\n",
    "#         clip_batch = clip_extractor.embed_image(img_batch)\n",
    "#         for clip in clip_batch:\n",
    "#             clip = clip.cpu().numpy()\n",
    "#             img_path = img_paths[idxs[running_count]]\n",
    "#             clip_file_name = str(img_path)[-24:-3]+\"npy\"\n",
    "#             np.save(os.path.join(training_clip_dir, clip_file_name), clip)\n",
    "#             running_count+=1\n",
    "        \n",
    "        \n",
    "# img2clipfile(train_imgs_paths, train_imgs_dataloader, idxs_train, training_clip_dir)\n",
    "# img2clipfile(train_imgs_paths, val_imgs_dataloader, idxs_val, validation_clip_dir)\n",
    "# img2clipfile(test_imgs_paths, test_imgs_dataloader, idxs_test, test_clip_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Clip2Vert and Vert2CLip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclePrototype(nn.Module):\n",
    "    def __init__(self, brain_dim, clip_dim):\n",
    "        super().__init__()\n",
    "        self.brain_dim = dict(out_dim=brain_dim, in_dim=clip_dim)\n",
    "        self.clip_dim = dict(out_dim=clip_dim, in_dim=brain_dim)\n",
    "        self.clip2vert = BrainNetwork(**self.brain_dim)\n",
    "        self.vert2clip = BrainNetwork(**self.clip_dim)\n",
    "    def forward(self, x, paired=False):\n",
    "        brain = self.clip2vert(x)\n",
    "        x = self.vert2clip(brain)\n",
    "        # whether want both brain activations and clip embedding\n",
    "        if paired:\n",
    "            return brain, x\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of cycle:\n",
      "param counts:\n",
      "263,527,832 total\n",
      "263,527,832 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "brain_dim = 15000\n",
    "clip_dim = 768\n",
    "cycle_kwargs = dict(brain_dim=brain_dim, clip_dim=clip_dim)\n",
    "cycle = CyclePrototype(**cycle_kwargs)\n",
    "\n",
    "print(\"params of cycle:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(cycle)\n",
    "    \n",
    "no_decay = ['bias']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in cycle.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in cycle.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=3e-4) # lr doesnt get used if lr_scheduler='cycle'\n",
    "\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    state_dict = cycle.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': cycle.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'lrs': lrs,\n",
    "        }, ckpt_path)\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Huggingface Accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algonauts images\n",
    "cycle, optimizer, train_alg_dataloader, val_alg_dataloader, test_alg_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_alg_dataloader, val_alg_dataloader, test_alg_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# COCO images\n",
    "cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing starting with epoch 0 / 120\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00719761848449707,
       "initial": 0,
       "n": 0,
       "ncols": 1200,
       "nrows": 27,
       "postfix": null,
       "prefix": "epochs",
       "rate": null,
       "total": 120,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b3d3eff6ef406190465af9b2a89fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004708528518676758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "training",
       "rate": null,
       "total": 868,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c657034188f044fabddb6f6a505785c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007710456848144531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "validation",
       "rate": null,
       "total": 97,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ef13a3b0264675991d33a1ee51ca94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /scratch/gpfs/dw26/fMRI-Algonauts-Challenge-2023/train_logs/testing/last.pth\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008642435073852539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "training",
       "rate": null,
       "total": 868,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ab523dc27d42589f19fce15f969398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007699489593505859,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "validation",
       "rate": null,
       "total": 97,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4815c57dab4372a94bb976a221caac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /scratch/gpfs/dw26/fMRI-Algonauts-Challenge-2023/train_logs/testing/last.pth\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010715723037719727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "training",
       "rate": null,
       "total": 868,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd7f6b0c9a74a86be6af479cf38cf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007726192474365234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "validation",
       "rate": null,
       "total": 97,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639464c0307c448589546fdc8d2134c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /scratch/gpfs/dw26/fMRI-Algonauts-Challenge-2023/train_logs/testing/last.pth\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007985830307006836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "training",
       "rate": null,
       "total": 868,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3d1eeaca9542188c2bbb41b0cebb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/pathlib.py:347\u001b[0m, in \u001b[0;36m_PosixFlavour.resolve.<locals>._resolve\u001b[0;34m(path, rest)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43maccessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/pathlib.py:452\u001b[0m, in \u001b[0;36m_NormalAccessor.readlink\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadlink\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument: '/scratch/gpfs/KNORMAN/COCO/unlabeled2017_all/21741.jpg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     21\u001b[0m     cycle\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_imgs_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#         repeat_index = train_i % 3\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/data_loader.py:388\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 388\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:668\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    672\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:706\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    705\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    708\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs_paths[idx]\n\u001b[0;32m---> 27\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2948\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2946\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fp, Path):\n\u001b[0;32m-> 2948\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2949\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m isPath(fp):\n\u001b[1;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/pathlib.py:1181\u001b[0m, in \u001b[0;36mPath.resolve\u001b[0;34m(self, strict)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_closed()\n\u001b[0;32m-> 1181\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flavour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;66;03m# No symlink resolution => for consistency, raise an error if\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;66;03m# the path doesn't exist or is forbidden\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/pathlib.py:363\u001b[0m, in \u001b[0;36m_PosixFlavour.resolve\u001b[0;34m(self, path, strict)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# NOTE: according to POSIX, getcwd() cannot contain path components\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# which are symlinks.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_absolute() \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m sep\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/pathlib.py:347\u001b[0m, in \u001b[0;36m_PosixFlavour.resolve.<locals>._resolve\u001b[0;34m(path, rest)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Resolve the symbolic link\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43maccessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m EINVAL \u001b[38;5;129;01mand\u001b[39;00m strict:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth')\n",
    "    epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    cycle.load_state_dict(checkpoint['model_state_dict'])\n",
    "    losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    \n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0), desc='epochs')\n",
    "for epoch in progress_bar:\n",
    "    cycle.train()\n",
    "    cocoRate = 1 # int number of COCO batches to algonauts batches\n",
    "    alpha = .5 # fraction of total loss due to brain space loss for algonauts batches\n",
    "    alg_batches = enumerate(tqdm(train_alg_dataloader, desc='training algonauts'))\n",
    "    for train_i, coco_image in enumerate(tqdm(train_coco_dataloader, desc='training coco')):\n",
    "        if train_i%cocoRate==0:\n",
    "            img_batches = [coco_image, alg_batches.next()]\n",
    "        else:\n",
    "            img_batches = [coco_image]\n",
    "        for j, data in enumerate(img_batches):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        #         repeat_index = train_i % 3\n",
    "            if j!=0:\n",
    "                alg_index, image, lh_vert, rh_vert = data\n",
    "\n",
    "            image = image.float()\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "            if j!=0:\n",
    "                brain_target = torch.cat(lh_vert, rh_vert)\n",
    "                brain_pred, clip_pred = cycle(clip_target, paired=True) \n",
    "                brain_loss = mse(brain_target, brain_pred)\n",
    "                clip_loss = mse(clip_target, clip_pred)\n",
    "                loss = (1-alpha)*clip_loss+alpha*brain_loss\n",
    "            else:\n",
    "                brain_pred, clip_pred = cycle(clip_target) \n",
    "                loss = mse(clip_target, clip_cycle)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    cycle.eval()\n",
    "    if local_rank==0: # i think its possible to remove this if statement though with some revisions\n",
    "        for val_i, image in enumerate(tqdm(val_imgs_dataloader, desc='validation')): \n",
    "            with torch.no_grad():\n",
    "                # repeat_index = val_i % 3\n",
    "\n",
    "                image = image.float()\n",
    "\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "                clip_cycle = cycle(clip_target) \n",
    "                val_loss =  mse(clip_target, clip_cycle)\n",
    "                utils.check_loss(val_loss)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        if (not save_at_end and not no_ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "        \n",
    "        # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            save_ckpt(f'last')\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"val/num_steps\": len(val_losses)}\n",
    "        \n",
    "        progress_bar.set_postfix(**logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2887\n",
      "291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14593e4dd0d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbmklEQVR4nO3deXRcZ5nn8e9Ti0qrLdmSY8ebnNXjLCRBkwUYSKBDJ2EJoenpQB8SliF9aGgCwyxhCzTN0jTTcIZlyMmQNEk3JIEGQug4hNAEAgMklhPb8RInTvC+yZZsay2pVM/8UdeO7GgpySXdurd+n3Pq6Na9V1XP6yv/dPXWe+9r7o6IiFSGRNgFiIjIzFHoi4hUEIW+iEgFUeiLiFQQhb6ISAVJhfXGzc3N3traGtbbi4hE0urVqw+4e8tUvz+00G9tbaW9vT2stxcRiSQz23Yy36/uHRGRCqLQFxGpIAp9EZEKotAXEakgCn0RkQqi0BcRqSAKfRGRChK50N+8t5t//PlmOnsHwy5FRCRyIhf6L3T08PVfbmHfkYGwSxERiZzIhf5Ra3YcCrsEEZHIiVzob+/sA+B/Pbw55EpERKIncqH/l5cuBeCGy1rDLUREJIIiF/p1VUlSCSObGw67FBGRyIlc6JsZ9dUpugdyYZciIhI5kQt9gDm1VXT2acimiMhkRTL0m+szHOjOhl2GiEjkhDaJyskYyud5avsh8nknkbCwyxERiYxInumfe+psAJ7edTjkSkREoiWSoX/dRQsBdCsGEZFJimToz6pOA3BkYCjkSkREoiWioV/4KOKIhm2KiExKNEO/pnCm/6n714dciYhItEwY+ma22MweNbONZrbBzG4eZZ/Lzeywma0JHrdOT7kFmVQkf1eJiISumCGbOeCj7v6kmTUAq83sEXffeMJ+v3H3N5a+xJcyMxpr09RnIjniVEQkNBOmprvvAfYEy91mtglYCJwY+jPqUN8Qh/r0Qa6IyGRMqp/EzFqBC4HHR9l8mZmtNbOHzOycUhRXjKHh/Ey9lYhI5BUd+mZWD/wQ+LC7Hzlh85PAUnd/GfB14P4xXuMmM2s3s/aOjo4pllzwt28u/F453K+zfRGRYhUV+maWphD433X3H5243d2PuHtPsLwSSJtZ8yj73e7ube7e1tLSclKFzw5G8KiLR0SkeMWM3jHgDmCTu39ljH3mB/thZhcHr3uwlIWeqDqdBGC9bsUgIlK0Yoa/vBJ4J/C0ma0J1n0cWALg7rcBbwPeb2Y5oB+43t299OW+aGFjDQAPrd/DWy5cOJ1vJSISG8WM3vktMO6tLN39G8A3SlVUMc5bNJvG2jRNtVUz+bYiIpEW6auc5jVk1KcvIjIJkQ792TVpjd4REZmEiId+FV2aNlFEpGiRDv3m+irdU19EZBIiHfpz6gqhP80DhUREYiPSoT+3PkMu7xzp1331RUSKEenQP3pV7vbOvpArERGJhkiHfl1V4arcx/84rRf/iojERqRD/7xFs4EX58wVEZHxRTr0a4L77/QPDYdciYhINEQ79KsU+iIikxHp0K9OFUK/b1ChLyJSjEiHfiJh1KST9GU1ZFNEpBiRDn0oXKB1UFfliogUJfKh39yQ4UBPNuwyREQiIfKh36g7bYqIFC3yoV9fnaJnQH36IiLFiHzoN2RSdOuDXBGRokQ+9OszOtMXESlW9EO/OkX/0DC54XzYpYiIlL3oh36mMLd7b1YXaImITCTyod9QXQj97qxG8IiITCQGoV+4w2aPPswVEZlQ5EP/aPeOPswVEZlY9EP/WPeOQl9EZCKRD/0GnemLiBQt8qF/9ExfffoiIhOLfujrTF9EpGgThr6ZLTazR81so5ltMLObR9nHzOxrZrbFzNaZ2UXTU+5L1VWpT19EpFipIvbJAR919yfNrAFYbWaPuPvGEftcDZwZPC4BvhV8nXaJhOlWDCIiRZrwTN/d97j7k8FyN7AJWHjCbtcCd3vBH4BGM1tQ8mrHUJ9J0aOLs0REJjSpPn0zawUuBB4/YdNCYMeI5zt56S+GaVNfnaJbZ/oiIhMqOvTNrB74IfBhdz8ylTczs5vMrN3M2js6OqbyEqMqnOkr9EVEJlJU6JtZmkLgf9fdfzTKLruAxSOeLwrWHcfdb3f3Nndva2lpmUq9o2rQmb6ISFGKGb1jwB3AJnf/yhi7PQDcEIziuRQ47O57SljnuHSmLyJSnGJG77wSeCfwtJmtCdZ9HFgC4O63ASuBa4AtQB/w7pJXOg6N3hERKc6Eoe/uvwVsgn0c+ECpipqs+mqd6YuIFCPyV+RC4fbKPdkc+byHXYqISFmLR+gfnT1rUGf7IiLjiUXo66ZrIiLFiUfo66ZrIiJFiUfoayIVEZGixCL0NZGKiEhxYhH66tMXESlOPEI/ONPfd2Qg5EpERMpbLEK/sbYKgF8/W7qbuImIxFEsQv/omX5zfSbkSkREylssQh/g1NnVYZcgIlL2YhP61VVJ+oeGwy5DRKSsxSf0U0myCn0RkXHFJvRrdKYvIjKh2IR+dTrBwFA+7DJERMpabEK/Jp1kQGf6IiLjik3oZ9Lq3hERmUhsQr+5roqOI9mwyxARKWuxCf1ZNWl6BnMUZm4UEZHRxCb0M6kE7jA0rNAXERlLjEI/CUA2p359EZGxxCb0q1KFpgzmNGxTRGQssQn9TBD6WYW+iMiY4hP6aYW+iMhE4hP6QZ++LtASERlbbEJ/VnUagCP9QyFXIiJSvmIT+k11hdDv6hsMuRIRkfIVn9APpkzs6tOZvojIWCYMfTO708z2m9n6MbZfbmaHzWxN8Li19GVO7Gjod/bqTF9EZCypIvb5DvAN4O5x9vmNu7+xJBVNUU1Vkup0gkPq3hERGdOEZ/ru/hjQOQO1nLSBoTzfb98ZdhkiImWrVH36l5nZWjN7yMzOKdFrTslhjd4RERlTKUL/SWCpu78M+Dpw/1g7mtlNZtZuZu0dHR0leOvjLWuuK/lriojEyUmHvrsfcfeeYHklkDaz5jH2vd3d29y9raWl5WTf+iUuPW0OLQ2Zkr+uiEhcnHTom9l8M7Ng+eLgNQ+e7OtORbWmTBQRGdeEo3fM7B7gcqDZzHYCnwbSAO5+G/A24P1mlgP6ges9pJlMFPoiIuObMPTd/e0TbP8GhSGdoatNJxkadgaGhqlOJ8MuR0Sk7MTmilyAhU01AOw+1B9yJSIi5SlWoV+XKfzh0jeoLh4RkdHEKvRrqwpdOgp9EZHRxSz0j57p50KuRESkPMUs9Atn+u/6p1UhVyIiUp5iFfqnteiKXBGR8cQq9I9OmSgiIqMr5tbKkVKTTnLW/IawyxARKUuxOtMH6B8aZu2OQ2GXISJSlmIX+iIiMrbYhv7+IwNhlyAiUnZiF/qL5xRuxfCD1ZpBS0TkRLEL/QWzCqG/amskZngUEZlRsQv9W9+0AoDLzyr9JC0iIlEXu9BfMrcWgN9uCWUeFxGRsha70K8L7r/zi037Qq5ERKT8xC70kwkLuwQRkbIVu9AXEZGxxTr0B3P5sEsQESkrsQ79IwNDYZcgIlJWYh36O7s0V66IyEixDv0vPfRM2CWIiJSVWIb+b/7HFQC8dvm8kCsRESkvsQz9loYMAEN5fZArIjJSLEM/k0qQTBg9A5ogXURkpFiGvplxSkOG3Yf0Qa6IyEixDH2A3YcHuH/Nbtw97FJERMpGbEP/qBv/aVXYJYiIlI0JQ9/M7jSz/Wa2foztZmZfM7MtZrbOzC4qfZlT99izHWGXICJSNoo50/8OcNU4268GzgweNwHfOvmyTt4X33pe2CWIiJSdCUPf3R8DxpuG6lrgbi/4A9BoZgtKVeBUXbC48dhyt27HICIClKZPfyGwY8TzncG6UC2f33Bs+XfPa0IVERGY4Q9yzewmM2s3s/aOjuntazd78b76//OH66b1vUREoqIUob8LWDzi+aJg3Uu4++3u3ububS0tMzeH7aE+de+IiEBpQv8B4IZgFM+lwGF331OC1z1p//LeS8IuQUSkrKQm2sHM7gEuB5rNbCfwaSAN4O63ASuBa4AtQB/w7ukqdrJedWbzseVsbphMKhliNSIi4Zsw9N397RNsd+ADJatomjy3r4dzF84OuwwRkVDF/orct15YGEj0g/YdE+wpIhJ/sQ/9W9+0AoDazIR/1IiIxF7sQ39WdRqAb/3q+ZArEREJX+xDP5F4cby+JkoXkUoX+9Afaf+RgbBLEBEJVUWF/kfuWxt2CSIioaqI0P/8decC8PSuwyFXIiISrooI/XdcvASAv2hbPMGeIiLxVhGhb2YsmVNLNjccdikiIqGqiNAH2N7Zx/1rdnO4XyN4RKRyVUzoH3WgJxt2CSIioamY0F8wuxqAb//mhZArEREJT8WE/rUXFO7Bs6OzP+RKRETCUzGh/zevPQOA/iF9mCsilatiQr+2qnAv/dXbukKuREQkPBUT+iPnzB3Q2b6IVKiKCX2Ad72iFYAfPTnqFL4iIrFXUaG/dG4tAB//8dMhVyIiEo6KCv0bL2sNuwQRkVBVVOiPvLd++9bOECsREQlHRYU+wB03tgHwpZ89E3IlIiIzr+JC/4qz5wGwamsXv9tyIORqRERmVsWFfiJh/OUlhVstv+PbjzOYy4dckYjIzKm40Af43FvOPbb80R9oNi0RqRwVGfpmxoMfehUAP127m3/ftC/kikREZkZFhj7AOafO5sIljQC896523X1TRCpCxYY+wPf/6rJjy597cBMbdmsOXRGJt4oO/XQywfNfuObY8zd87bdsO9gbYkUiItOrqNA3s6vMbLOZbTGzW0bZ/i4z6zCzNcHjv5S+1OmRTNhxwf+aL/8qvGJERKbZhKFvZkngm8DVwArg7Wa2YpRd73P3C4LHt0tc57RKJowtn7/62PPzPv0wz+7rDrEiEZHpUcyZ/sXAFnd/wd0HgXuBa6e3rJmXSib43S2vBaA7m+P1X31MY/hFJHaKCf2FwI4Rz3cG6070Z2a2zsz+1cwWj/ZCZnaTmbWbWXtHR8cUyp1epzbW8Nlrzzn2/KxPPqR79IhIrJTqg9yfAq3ufj7wCHDXaDu5++3u3ububS0tLSV669K64bJWPhRMrQjwttt+z0/W7MLdQ6xKRKQ0ign9XcDIM/dFwbpj3P2gu2eDp98GXl6a8sLxX19/Nj/+61cce37zvWtY9rGVDOcV/CISbcWE/irgTDNbZmZVwPXAAyN3MLMFI56+GdhUuhLDceGSpuNG9QCc/vGVbD/YF1JFIiInb8LQd/cc8EHgYQph/n1332BmnzWzNwe7fcjMNpjZWuBDwLumq+CZlEwY93/glWRSL/4zvfrLj2osv4hEloXVV93W1ubt7e2hvPdU/IdP/Yz+EyZU/+MXrzluwnURkelmZqvdvW2q31/RV+ROxqa/u4qbX3fmceuWfWwlew73h1SRiMjk6Ux/koaG81z5lV+z9YS+/c2fu4pMKhlSVSJSKXSmP8PSyQS/+u9XsOmzVx23/uxP/kw3bBORsqfQn6KaqiTPff5qPnb18mPr3vC139J6y4O6kldEypZC/ySkkwn+6jWn8+Snrjxu/VmffIh/W7c7pKpERMam0C+BOXVVbP37N/DKM+YeW/fB7z1F6y0PaniniJQVhX4J3f2eS/iHPzv/uHWv+fKvaL3lwZAqEhE5nkbvTJOfrt3N39zz1HHrXrZoNq9dfgo3/8mZY3yXiMj4NHqnTL3pZafyxy9ew5UrTjm2bu3Ow3z1F8/SesuD9GZzIVYnIpVKZ/oz5JfP7OM93xm9vU996kqa6qpmuCIRiaKTPdNX6M+wx57t4IY7n3jJ+qpUgnvedykvX9oUQlUiEhUK/Yh6ansX1/2f3425/YlPvI55DdUzWJGIRIFCPwbufWI7t/zo6VG3ffTKs/jAFWeQSOjGbiKi0I+dFzp6+OD3nmLjniOjbv/5R17NwsYa6jKpGa5MRMqBQj+mntl7hPfd3c6OzvHv4vnFt57H2y9eMkNViUjYFPoVYOPuIzy5vYtP3r9+3P2a66v41BtX0NY6h4WNNTNUnYjMJIV+BRoYGuaLKzdxz6odE97cbVlzHZ+/7lxecXrzDFUnItNJoS/0Deb4lz9s4wsrnyn6e5Y113HXuy+mJ5tj+fwGfVAsEhEKfXmJbG4Yd3hk4z4+/cAGOnsHi/7ed72ilf/YOocLljRySkOGVFIXbYuUE4W+FC2bG2brgT4+ct+aMUcHjSdhkHd4+dImPvInZ7FmRxevOrOFCxY3lr5YERmVQl9OSj7v9A8Nk83leXjDXp7edZjvPb79pF/35UubWDKnliuWz+N1y+dRnU6SDLqQ3F0TyotMkUJfpk02N0zSjFzeeWTjPlZv62LtzkM8tf1Qyd/r4mVzMGBRUy2ntdSxZE4tC5tqSJjROreWmqok6UQCh2O/PEQqkUJfysKBnizpZIKu3kHuXbWDRzbu5fmOmZtAZsHsavYcHuD0ljrecP6pNNWmWfn0HjKpJO+4ZAktDRkyqQQtDRkaa6pIJY3+oWFmVadnrEaRUlDoS6Qc/XnL5vLsOTzAwxv2knenIZPi+Y5evvO7reEWOIF5DRn2d2f5z22L+H77Tj529XL6BocZzju/f+Egn7/uXDKpJFsP9nJGSz2dvYPs787yitPnkjCjJ5ujKpUgnTRq0kkAdXXJpCj0peJ0DwyRTibo7B2ks3eQdTsPs6CxmrU7DnHHb/5IdzbHigWzpvRhdTlZ1FTDzq7xr8gGaGnI0NGd5ZxTZ1GXSZFOGv9vy0He95+W8ZM1u7nktLnMqk5xamMNew73k04m6M3muOLseaza2sW5C2dxsGeQ+bOrWTq3lr2HB7hgcSOrtnYxp66K0+fVMTCYpzs7xKzqNHPqqqhKJdh3ZIBFTbXH6ujN5nR7kBmg0BcpgdxwnrxD/9Aw+byTMGNbZy9L59TR1TfIUzu6AOgbHGbrgV6e3H6IAz1ZmuszrN7WRXU6QTaX54qz5/HLZ/aH3JryNreuioOTGEY8lhULZlGfSfHE1k4Azl80m3U7Dx/bXpVKHLt4sa4qyeVnz+O5/d3UVKVoqk3zq80dx+r5zJvPYdOeI/QNDpNJJfj9Cwc5b+FsTm2s4debO5g/u5rebI4/b1vMrkP95PPO3PoqzpzXwAsHejgykOPU2dVsO9jHWac0sLCphvpMis17u2mqSzMwlKexNk3PQI7tnX1cfnYLDVPsWlToi8RIPu8MuzOcd6rTSdydoWEn707/4DDpIMh6szlaGjJs7+zjYM8gXX2DPLO3m7qqJIvn1LL1YC/nL2zknlXbaanPsPtQP7/YtI988N99+fwGzphXz7+t28Py+Q08s7cbKEzpuXZEcMr0eMclS/jCdedN6XsV+iJSdgaGho/90oKXfm6RzQ1jGAmDVDJBPu/0DuZoqE4zmMuTdyebyzOrOsXzHT3MqcswnHc6urM0VKfoGxzmlFkZDvYOsu1gL7NrqsikEuzs6qNvcJjewWEubp3DIxv3ctnpzRzsyTI07Kxcv4eW+sJrJRPG/NnV/PKZ/bzmrBYO9Q2SzeVJmNHVN8i8hgw1VSl+/NROmmqrqKtKsayljnU7D7HtQB/d2RzvvHQp63YeYm59hhc6eth6sA8oXPG+vbOP4bzTUJ3iwiVNNFSneHDdHq4+dz5fuO68Kc+WNyOhb2ZXAf8bSALfdve/P2F7BrgbeDlwEPgLd9863msq9EVEJm/aJ0Y3syTwTeBqYAXwdjNbccJu7wW63P0M4KvAl6ZakIiITJ9ibqxyMbDF3V9w90HgXuDaE/a5FrgrWP5X4HWmcWgiImWnmNBfCOwY8XxnsG7Ufdw9BxwG5p74QmZ2k5m1m1l7R0fH1CoWEZEpm9FbKLr77e7e5u5tLS0tM/nWIiJCcaG/C1g84vmiYN2o+5hZCphN4QNdEREpI8WE/irgTDNbZmZVwPXAAyfs8wBwY7D8NuCXHtZYUBERGdOE10y7e87MPgg8TGHI5p3uvsHMPgu0u/sDwB3AP5vZFqCTwi8GEREpM0XdKMPdVwIrT1h364jlAeDPS1uaiIiUWmhX5JpZB7Btit/eDBwoYTnlQG2Khri1KW7tgfi3aam7T3kkTGihfzLMrP1krkgrR2pTNMStTXFrD6hNE9Gs1yIiFUShLyJSQaIa+reHXcA0UJuiIW5tilt7QG0aVyT79EVEZGqieqYvIiJToNAXEakgkQt9M7vKzDab2RYzuyXseibDzLaa2dNmtsbM2oN1c8zsETN7LvjaFKw3M/ta0M51ZnZRuNWDmd1pZvvNbP2IdZOu38xuDPZ/zsxuHO29ZsoYbfqMme0KjtMaM7tmxLaPBW3abGZ/OmJ92fxcmtliM3vUzDaa2QYzuzlYH8ljNU57InuczKzazJ4ws7VBm/42WL/MzB4P6rsvuPUNZpYJnm8JtreOeK1R2zomd4/Mg8JtIJ4HTgOqgLXAirDrmkT9W4HmE9b9A3BLsHwL8KVg+RrgIcCAS4HHy6D+VwMXAeunWj8wB3gh+NoULDeVWZs+A/y3UfZdEfzMZYBlwc9istx+LoEFwEXBcgPwbFB7JI/VOO2J7HEK/q3rg+U08Hjwb/994Ppg/W3A+4PlvwZuC5avB+4br63jvXfUzvSLmdAlakZOQHMX8JYR6+/2gj8AjWa2IIT6jnH3xyjcW2mkydb/p8Aj7t7p7l3AI8BV0178GMZo01iuBe5196y7/xHYQuFnsqx+Lt19j7s/GSx3A5sozHkRyWM1TnvGUvbHKfi37gmepoOHA6+lMBEVvPQYjTZR1VhtHVPUQr+YCV3KmQM/N7PVZnZTsO4Ud98TLO8FTgmWo9LWydYflXZ9MOjquPNoNwgRbFPQDXAhhTPJyB+rE9oDET5OZpY0szXAfgq/UJ8HDnlhIio4vr6xJqqadJuiFvpR9yp3v4jCfMMfMLNXj9zohb/XIjuGNur1j/At4HTgAmAP8I+hVjNFZlYP/BD4sLsfGbktisdqlPZE+ji5+7C7X0BhjpKLgeUz8b5RC/1iJnQpW+6+K/i6H/gxhQO972i3TfB1f7B7VNo62frLvl3uvi/4D5kH/i8v/rkcmTaZWZpCQH7X3X8UrI7ssRqtPXE4TgDufgh4FLiMQtfa0bsfj6xvrImqJt2mqIV+MRO6lCUzqzOzhqPLwOuB9Rw/Ac2NwE+C5QeAG4KRFZcCh0f8aV5OJlv/w8Drzawp+HP89cG6snHCZyfXUThOUGjT9cFIimXAmcATlNnPZdDXewewyd2/MmJTJI/VWO2J8nEysxYzawyWa4ArKXxW8SiFiajgpcdotImqxmrr2ML45PpkHhRGGjxLof/rE2HXM4m6T6PwKftaYMPR2in0y/078BzwC2COv/jp/jeDdj4NtJVBG+6h8Gf0EIW+w/dOpX7gPRQ+cNoCvLsM2/TPQc3rgv9UC0bs/4mgTZuBq8vx5xJ4FYWum3XAmuBxTVSP1TjtiexxAs4HngpqXw/cGqw/jUJobwF+AGSC9dXB8y3B9tMmautYD92GQUSkgkSte0dERE6CQl9EpIIo9EVEKohCX0Skgij0RUQqiEJfRKSCKPRFRCrI/wfRbHAq581DoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(losses))\n",
    "print(len(val_losses))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14593e4ac0a0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
