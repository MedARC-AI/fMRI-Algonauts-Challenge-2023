{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-dweisberg/miniconda3/envs/medical-v1/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# get custom models and functions\n",
    "import sys\n",
    "# sys.path.append('../fMRI-reconstruction-NSD/src')\n",
    "sys.path.append('/fsx/proj-medarc/fmri/fMRI-reconstruction-NSD/src/')\n",
    "from models import Clipper, BrainNetwork\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Iprogress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mIprogress\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Iprogress'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm # for jupyter notebook\n",
    "from accelerate import Accelerator\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import Iprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-dweisberg/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '../algonauts_2023_challenge_data'\n",
    "data_dir = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/dataset\"\n",
    "# parent_submission_dir = 'algonauts_2023_challenge_submission'\n",
    "parent_submission_dir = \"/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/algonauts_2023_challenge_submission\"\n",
    "\n",
    "subj = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=testing', '--clip_variant=ViT-L/14', '--batch_size=128']\n"
     ]
    }
   ],
   "source": [
    "# can specify jupyter_args here for argparser to use if running this code interactively\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=testing\")\n",
    "    jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    jupyter_args.append(\"--batch_size=128\") # smaller to account for more loaded models.\n",
    "#     jupyter_args.append(\"--resume_from_ckpt\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if not using wandb and want to resume from a ckpt\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--resume_from_ckpt',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if not using wandb and want to resume from a ckpt.',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--no_ckpt_saving',\n",
    "    action='store_false',\n",
    "    default=True,\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=1,\n",
    "    help=\"save ckpt every x epochs\",\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--save_at_end',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if False, will save best.ckpt whenever epoch shows best validation score',\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--seed\",type=int,default=42,\n",
    "# )\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Models and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for dataloaders\n",
    "\n",
    "def _transforms(n_px=256, reshape=False):\n",
    "    if reshape:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.CenterCrop(n_px),\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    return transform\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            n_px = min(img.size)\n",
    "            img = self.transform(n_px, reshape=True)(img).to(device)\n",
    "        else:\n",
    "            img = _transforms()(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training COCO images: 111063\n",
      "\n",
      "Validation COCO images: 12340\n"
     ]
    }
   ],
   "source": [
    "# coco_dir = \"/scratch/gpfs/KNORMAN/COCO/unlabeled2017_all\"\n",
    "coco_dir = \"/fsx/proj-medarc/fmri/coco/unlabeled2017\"\n",
    "\n",
    "# Create list with all file names, sorted\n",
    "coco_list = os.listdir(coco_dir)\n",
    "coco_list.sort()\n",
    "\n",
    "rand_seed = 5 \n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Calculate how many COCO images correspond to 90% for training\n",
    "num_train_coco = int(np.round(len(coco_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs_coco = np.arange(len(coco_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train_coco, idxs_val_coco = idxs_coco[:num_train_coco], idxs_coco[num_train_coco:]\n",
    "\n",
    "print('Training COCO images: ' + format(len(idxs_train_coco)))\n",
    "print('\\nValidation COCO images: ' + format(len(idxs_val_coco)))\n",
    "\n",
    "# Get the paths of all image files\n",
    "coco_paths = sorted(list(Path(coco_dir).iterdir()))\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_train_coco, _transforms), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_val_coco, _transforms), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Algonauts Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Get Algonauts fMRI data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')\n",
    "\n",
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]\n",
    "\n",
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algonauts Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, lh_fmri, rh_fmri, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "        self.lh_fmri = lh_fmri\n",
    "        self.rh_fmri = rh_fmri\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            n_px = min(img.size)\n",
    "            img = self.transform(n_px, reshape=True)(img).to(device)\n",
    "        else:\n",
    "            img = _transforms()(img)\n",
    "        return img, torch.from_numpy(self.lh_fmri[idx]), torch.from_numpy(self.rh_fmri[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# Algonauts dataloaders\n",
    "train_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_train, lh_fmri_train, rh_fmri_train), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_val, lh_fmri_val, rh_fmri_val), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "# just images\n",
    "test_alg_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# training_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/training_clip\"\n",
    "# validation_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/validation_clip\"\n",
    "# test_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/test_split/test_clip\"\n",
    "\n",
    "# if not os.path.isdir(training_clip_dir):\n",
    "#     os.makedirs(training_clip_dir)\n",
    "# if not os.path.isdir(validation_clip_dir):\n",
    "#     os.makedirs(validation_clip_dir)\n",
    "# if not os.path.isdir(test_clip_dir):\n",
    "#     os.makedirs(test_clip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cpu\n"
     ]
    }
   ],
   "source": [
    "# get clipper\n",
    "clip_extractor = Clipper(\"ViT-L/14\", device=torch.device(device)) # run in terminal to download\n",
    "\n",
    "# def img2clipfile(img_paths, dataloader, idxs, clip_dir):\n",
    "#     running_count=0\n",
    "#     for img_batch in tqdm(dataloader):\n",
    "#         clip_batch = clip_extractor.embed_image(img_batch)\n",
    "#         for clip in clip_batch:\n",
    "#             clip = clip.cpu().numpy()\n",
    "#             img_path = img_paths[idxs[running_count]]\n",
    "#             clip_file_name = str(img_path)[-24:-3]+\"npy\"\n",
    "#             np.save(os.path.join(training_clip_dir, clip_file_name), clip)\n",
    "#             running_count+=1\n",
    "        \n",
    "        \n",
    "# img2clipfile(train_imgs_paths, train_imgs_dataloader, idxs_train, training_clip_dir)\n",
    "# img2clipfile(train_imgs_paths, val_imgs_dataloader, idxs_val, validation_clip_dir)\n",
    "# img2clipfile(test_imgs_paths, test_imgs_dataloader, idxs_test, test_clip_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Clip2Vert and Vert2CLip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclePrototype(nn.Module):\n",
    "    def __init__(self, brain_dim, clip_dim):\n",
    "        super().__init__()\n",
    "        self.brain_dim = dict(out_dim=brain_dim, in_dim=clip_dim)\n",
    "        self.clip_dim = dict(out_dim=clip_dim, in_dim=brain_dim)\n",
    "        self.clip2vert = BrainNetwork(**self.brain_dim)\n",
    "        self.vert2clip = BrainNetwork(**self.clip_dim)\n",
    "    def forward(self, x, paired=False):\n",
    "        brain = self.clip2vert(x)\n",
    "        x = self.vert2clip(brain)\n",
    "        # whether want both brain activations and clip embedding\n",
    "        if paired:\n",
    "            return brain, x\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain dimension: 39548\n",
      "CLIP dimension: 768\n",
      "params of cycle:\n",
      "param counts:\n",
      "464,649,596 total\n",
      "464,649,596 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "brain_dim = lh_fmri_train.shape[1]+rh_fmri_train.shape[1]\n",
    "clip_dim = 768\n",
    "cycle_kwargs = dict(brain_dim=brain_dim, clip_dim=clip_dim)\n",
    "cycle = CyclePrototype(**cycle_kwargs)\n",
    "print(\"Brain dimension:\", brain_dim)\n",
    "print(\"CLIP dimension:\", clip_dim)\n",
    "\n",
    "print(\"params of cycle:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(cycle)\n",
    "    \n",
    "no_decay = ['bias']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in cycle.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in cycle.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=3e-4) # lr doesnt get used if lr_scheduler='cycle'\n",
    "\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    state_dict = cycle.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': cycle.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'lrs': lrs,\n",
    "        }, ckpt_path)\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Huggingface Accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algonauts images\n",
    "cycle, optimizer, train_alg_dl, val_alg_dl, test_alg_dl, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_alg_dataloader, val_alg_dataloader, test_alg_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# COCO images\n",
    "cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing starting with epoch 0 / 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 0/120 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "training coco:   0%|          | 1/868 [02:20<33:55:53, 140.89s/it]\n",
      "epochs:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 0/120 [02:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     image \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m clip_target \u001b[39m=\u001b[39m clip_extractor\u001b[39m.\u001b[39;49membed_image(image)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m j\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e7465726e616c2e6870632e73746162696c6974792e6169222c2275736572223a22647765697362657267227d/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/cyclicPrototype.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     brain_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((lh_vert, rh_vert), \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/fsx/proj-medarc/fmri/fMRI-reconstruction-NSD/src/models.py:170\u001b[0m, in \u001b[0;36mClipper.embed_image\u001b[0;34m(self, image, apply_transforms, apply_spatial_transforms)\u001b[0m\n\u001b[1;32m    167\u001b[0m     clip_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversatile_process_embeddings(clip_emb)\n\u001b[1;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m clip_emb\n\u001b[0;32m--> 170\u001b[0m clip_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip\u001b[39m.\u001b[39;49mencode_image(clip_emb)\n\u001b[1;32m    171\u001b[0m \u001b[39m# input is now in CLIP space, but mind-reader preprint further processes embeddings:\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclamp_embs:\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:337\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image\u001b[39m.\u001b[39;49mtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:228\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    225\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    227\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    229\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    231\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post(x[:, \u001b[39m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:199\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:187\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    186\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[0;32m--> 187\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[1;32m    188\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/medical-v1/lib/python3.10/site-packages/clip/model.py:164\u001b[0m, in \u001b[0;36mQuickGELU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49msigmoid(\u001b[39m1.702\u001b[39;49m \u001b[39m*\u001b[39;49m x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth')\n",
    "    epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    cycle.load_state_dict(checkpoint['model_state_dict'])\n",
    "    losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    \n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0), desc='epochs')\n",
    "for epoch in progress_bar:\n",
    "    cycle.train()\n",
    "    cocoRate = 1 # int number of COCO batches to algonauts batches\n",
    "    alpha = .5 # fraction of total loss due to brain space loss for algonauts batches\n",
    "    alg_batches = enumerate(tqdm(train_alg_dl, desc='training algonauts'))\n",
    "    for train_i, coco_image in enumerate(tqdm(train_coco_dl, desc='training coco')):\n",
    "        if train_i%cocoRate==0:\n",
    "            alg_batch = next(alg_batches, None)\n",
    "            # if end of algonauts dataloader reached, start over for another epoch\n",
    "            if alg_batch is None:\n",
    "                alg_batches = enumerate(tqdm(train_alg_dl, desc='training algonauts'))\n",
    "                alg_batch = next(alg_batches)\n",
    "            img_batches = [coco_image, alg_batch]\n",
    "        else:\n",
    "            img_batches = [coco_image]\n",
    "        for j, data in enumerate(img_batches):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        #         repeat_index = train_i % 3\n",
    "            if j!=0:\n",
    "                alg_index, (image, lh_vert, rh_vert) = data\n",
    "            else:\n",
    "                image = data\n",
    "\n",
    "            image = image.float()\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "            if j!=0:\n",
    "                brain_target = torch.cat((lh_vert, rh_vert), 1)\n",
    "                brain_pred, clip_pred = cycle(clip_target, paired=True) \n",
    "                brain_loss = mse(brain_target, brain_pred)\n",
    "                clip_loss = mse(clip_target, clip_pred)\n",
    "                loss = (1-alpha)*clip_loss+alpha*brain_loss\n",
    "            else:\n",
    "                clip_pred = cycle(clip_target) \n",
    "                loss = mse(clip_target, clip_pred)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    cycle.eval()\n",
    "    if local_rank==0: # i think its possible to remove this if statement though with some revisions\n",
    "        for val_i, image in enumerate(tqdm(val_coco_dl, desc='validation')): \n",
    "            with torch.no_grad():\n",
    "                # repeat_index = val_i % 3\n",
    "\n",
    "                image = image.float()\n",
    "\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "                clip_cycle = cycle(clip_target) \n",
    "                val_loss =  mse(clip_target, clip_cycle)\n",
    "                utils.check_loss(val_loss)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        if (not save_at_end and not no_ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "        \n",
    "        # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            save_ckpt(f'last')\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"val/num_steps\": len(val_losses)}\n",
    "        \n",
    "        progress_bar.set_postfix(**logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '1 COCO batches(s) for every 1 algonauts batch')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(losses))\n",
    "print(len(val_losses))\n",
    "plt.plot(losses)\n",
    "plt.title(str(cocoRate)+\" COCO batches(s) for every 1 algonauts batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
