{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "# get custom models and functions\n",
    "import sys\n",
    "sys.path.append('../fMRI-reconstruction-NSD/src')\n",
    "from models import Clipper, BrainNetwork\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm # for jupyter notebook\n",
    "from accelerate import Accelerator\n",
    "import argparse\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../algonauts_2023_challenge_data'\n",
    "parent_submission_dir = 'algonauts_2023_challenge_submission'\n",
    "\n",
    "subj = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=testing', '--clip_variant=ViT-L/14', '--batch_size=128']\n"
     ]
    }
   ],
   "source": [
    "# can specify jupyter_args here for argparser to use if running this code interactively\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=testing\")\n",
    "    jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    jupyter_args.append(\"--batch_size=128\") # smaller to account for more loaded models.\n",
    "#     jupyter_args.append(\"--resume_from_ckpt\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if not using wandb and want to resume from a ckpt\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--resume_from_ckpt',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if not using wandb and want to resume from a ckpt.',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "# doesn't work in python 3.8\n",
    "# parser.add_argument(\n",
    "#     \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--no_ckpt_saving',\n",
    "    action='store_false',\n",
    "    default=True,\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=1,\n",
    "    help=\"save ckpt every x epochs\",\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "#     help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    "# )\n",
    "parser.add_argument(\n",
    "    '--save_at_end',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='if False, will save best.ckpt whenever epoch shows best validation score',\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--seed\",type=int,default=42,\n",
    "# )\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Models and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for dataloaders\n",
    "\n",
    "def _transforms(n_px=256, reshape=False):\n",
    "    if reshape:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.CenterCrop(n_px),\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "            ])\n",
    "    return transform\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            n_px = min(img.size)\n",
    "            img = self.transform(n_px, reshape=True)(img).to(device)\n",
    "        else:\n",
    "            img = _transforms()(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training COCO images: 111063\n",
      "\n",
      "Validation COCO images: 12340\n"
     ]
    }
   ],
   "source": [
    "coco_dir = \"/scratch/gpfs/KNORMAN/COCO/unlabeled2017_all\"\n",
    "\n",
    "# Create list with all file names, sorted\n",
    "coco_list = os.listdir(coco_dir)\n",
    "coco_list.sort()\n",
    "\n",
    "rand_seed = 5 \n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Calculate how many COCO images correspond to 90% for training\n",
    "num_train_coco = int(np.round(len(coco_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs_coco = np.arange(len(coco_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train_coco, idxs_val_coco = idxs_coco[:num_train_coco], idxs_coco[num_train_coco:]\n",
    "\n",
    "print('Training COCO images: ' + format(len(idxs_train_coco)))\n",
    "print('\\nValidation COCO images: ' + format(len(idxs_val_coco)))\n",
    "\n",
    "# Get the paths of all image files\n",
    "coco_paths = sorted(list(Path(coco_dir).iterdir()))\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_train_coco, _transforms), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_coco_dl = DataLoader(\n",
    "    ImageDataset(coco_paths, idxs_val_coco, _transforms), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Algonauts Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Get Algonauts fMRI data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')\n",
    "\n",
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]\n",
    "\n",
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algonauts Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, lh_fmri, rh_fmri, transform=None):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "        self.lh_fmri = lh_fmri\n",
    "        self.rh_fmri = rh_fmri\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            n_px = min(img.size)\n",
    "            img = self.transform(n_px, reshape=True)(img).to(device)\n",
    "        else:\n",
    "            img = _transforms()(img)\n",
    "        return img, torch.from_numpy(self.lh_fmri[idx]), torch.from_numpy(self.rh_fmri[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# Algonauts dataloaders\n",
    "train_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_train, lh_fmri_train, rh_fmri_train), \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "val_alg_dataloader = DataLoader(\n",
    "    AlgDataset(train_imgs_paths, idxs_val, lh_fmri_val, rh_fmri_val), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "# just images\n",
    "test_alg_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# training_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/training_clip\"\n",
    "# validation_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/training_split/validation_clip\"\n",
    "# test_clip_dir = \"/scratch/gpfs/dw26/algonauts_2023_challenge_data/subj01/test_split/test_clip\"\n",
    "\n",
    "# if not os.path.isdir(training_clip_dir):\n",
    "#     os.makedirs(training_clip_dir)\n",
    "# if not os.path.isdir(validation_clip_dir):\n",
    "#     os.makedirs(validation_clip_dir)\n",
    "# if not os.path.isdir(test_clip_dir):\n",
    "#     os.makedirs(test_clip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n"
     ]
    }
   ],
   "source": [
    "# get clipper\n",
    "clip_extractor = Clipper(\"ViT-L/14\", device=torch.device(device)) # run in terminal to download\n",
    "\n",
    "# def img2clipfile(img_paths, dataloader, idxs, clip_dir):\n",
    "#     running_count=0\n",
    "#     for img_batch in tqdm(dataloader):\n",
    "#         clip_batch = clip_extractor.embed_image(img_batch)\n",
    "#         for clip in clip_batch:\n",
    "#             clip = clip.cpu().numpy()\n",
    "#             img_path = img_paths[idxs[running_count]]\n",
    "#             clip_file_name = str(img_path)[-24:-3]+\"npy\"\n",
    "#             np.save(os.path.join(training_clip_dir, clip_file_name), clip)\n",
    "#             running_count+=1\n",
    "        \n",
    "        \n",
    "# img2clipfile(train_imgs_paths, train_imgs_dataloader, idxs_train, training_clip_dir)\n",
    "# img2clipfile(train_imgs_paths, val_imgs_dataloader, idxs_val, validation_clip_dir)\n",
    "# img2clipfile(test_imgs_paths, test_imgs_dataloader, idxs_test, test_clip_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Clip2Vert and Vert2CLip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclePrototype(nn.Module):\n",
    "    def __init__(self, brain_dim, clip_dim):\n",
    "        super().__init__()\n",
    "        self.brain_dim = dict(out_dim=brain_dim, in_dim=clip_dim)\n",
    "        self.clip_dim = dict(out_dim=clip_dim, in_dim=brain_dim)\n",
    "        self.clip2vert = BrainNetwork(**self.brain_dim)\n",
    "        self.vert2clip = BrainNetwork(**self.clip_dim)\n",
    "    def forward(self, x, paired=False):\n",
    "        brain = self.clip2vert(x)\n",
    "        x = self.vert2clip(brain)\n",
    "        # whether want both brain activations and clip embedding\n",
    "        if paired:\n",
    "            return brain, x\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain dimension: 39548\n",
      "CLIP dimension: 768\n",
      "params of cycle:\n",
      "param counts:\n",
      "464,649,596 total\n",
      "464,649,596 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "brain_dim = lh_fmri_train.shape[1]+rh_fmri_train.shape[1]\n",
    "clip_dim = 768\n",
    "cycle_kwargs = dict(brain_dim=brain_dim, clip_dim=clip_dim)\n",
    "cycle = CyclePrototype(**cycle_kwargs)\n",
    "print(\"Brain dimension:\", brain_dim)\n",
    "print(\"CLIP dimension:\", clip_dim)\n",
    "\n",
    "print(\"params of cycle:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(cycle)\n",
    "    \n",
    "no_decay = ['bias']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in cycle.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in cycle.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=3e-4) # lr doesnt get used if lr_scheduler='cycle'\n",
    "\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    state_dict = cycle.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': cycle.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'lrs': lrs,\n",
    "        }, ckpt_path)\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Huggingface Accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algonauts images\n",
    "cycle, optimizer, train_alg_dl, val_alg_dl, test_alg_dl, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_alg_dataloader, val_alg_dataloader, test_alg_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# COCO images\n",
    "cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler = accelerator.prepare(\n",
    "    cycle, optimizer, train_coco_dl, val_coco_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing starting with epoch 0 / 120\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029517650604248047,
       "initial": 0,
       "n": 0,
       "ncols": 1200,
       "nrows": 28,
       "postfix": null,
       "prefix": "epochs",
       "rate": null,
       "total": 120,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdc6abb7de646baa1446094bf02ecb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004622697830200195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a245de7952404496e2cb74482adf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02526569366455078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training coco",
       "rate": null,
       "total": 868,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1add7b3ac2a44abbc0dc9a3ad1f4e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training coco:   0%|          | 0/868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015284061431884766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d422983cb9418797176f4b46583103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009179830551147461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c798ea3fcaa4796816280d306c42f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009253501892089844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9417c820de604b37821e080c3ed95966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009102821350097656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99d6a1c7b2645679292cb131741b9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009178638458251953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554a009f832e48559172f900cb7cf0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009122371673583984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175acd4e7c5b437999530c2cca847c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009500980377197266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cb1a1cc3c14217a2fefcea723e4cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00916147232055664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4a9ccbbe1742d4914351301e0e03b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009026050567626953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e86f447689d42398177de1fb0610128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008875370025634766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d4aa1d91194655bba98e951101d4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009924888610839844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426d6ae0532435ba815796db254531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008826017379760742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "training algonauts",
       "rate": null,
       "total": 70,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe120316edf5401dbd77ca58023dcde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training algonauts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_imgs_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m cycle\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_rank\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# i think its possible to remove this if statement though with some revisions\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[43mval_imgs_dataloader\u001b[49m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)): \n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;66;03m# repeat_index = val_i % 3\u001b[39;00m\n\u001b[1;32m     72\u001b[0m             image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_imgs_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth')\n",
    "    epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    cycle.load_state_dict(checkpoint['model_state_dict'])\n",
    "    losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    \n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0), desc='epochs')\n",
    "for epoch in progress_bar:\n",
    "    cycle.train()\n",
    "    cocoRate = 1 # int number of COCO batches to algonauts batches\n",
    "    alpha = .5 # fraction of total loss due to brain space loss for algonauts batches\n",
    "    alg_batches = enumerate(tqdm(train_alg_dl, desc='training algonauts'))\n",
    "    for train_i, coco_image in enumerate(tqdm(train_coco_dl, desc='training coco')):\n",
    "        if train_i%cocoRate==0:\n",
    "            alg_batch = next(alg_batches, None)\n",
    "            # if end of algonauts dataloader reached, start over for another epoch\n",
    "            if alg_batch is None:\n",
    "                alg_batches = enumerate(tqdm(train_alg_dl, desc='training algonauts'))\n",
    "                alg_batch = next(alg_batches)\n",
    "            img_batches = [coco_image, alg_batch]\n",
    "        else:\n",
    "            img_batches = [coco_image]\n",
    "        for j, data in enumerate(img_batches):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        #         repeat_index = train_i % 3\n",
    "            if j!=0:\n",
    "                alg_index, (image, lh_vert, rh_vert) = data\n",
    "            else:\n",
    "                image = data\n",
    "\n",
    "            image = image.float()\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "            if j!=0:\n",
    "                brain_target = torch.cat((lh_vert, rh_vert), 1)\n",
    "                brain_pred, clip_pred = cycle(clip_target, paired=True) \n",
    "                brain_loss = mse(brain_target, brain_pred)\n",
    "                clip_loss = mse(clip_target, clip_pred)\n",
    "                loss = (1-alpha)*clip_loss+alpha*brain_loss\n",
    "            else:\n",
    "                clip_pred = cycle(clip_target) \n",
    "                loss = mse(clip_target, clip_pred)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    cycle.eval()\n",
    "    if local_rank==0: # i think its possible to remove this if statement though with some revisions\n",
    "        for val_i, image in enumerate(tqdm(val_imgs_dataloader, desc='validation')): \n",
    "            with torch.no_grad():\n",
    "                # repeat_index = val_i % 3\n",
    "\n",
    "                image = image.float()\n",
    "\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "                clip_cycle = cycle(clip_target) \n",
    "                val_loss =  mse(clip_target, clip_cycle)\n",
    "                utils.check_loss(val_loss)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        if (not save_at_end and not no_ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "        \n",
    "        # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            save_ckpt(f'last')\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"val/num_steps\": len(val_losses)}\n",
    "        \n",
    "        progress_bar.set_postfix(**logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(losses))\n",
    "print(len(val_losses))\n",
    "plt.plot(losses)\n",
    "plt.title(str(cocoRate)+\" COCO batches(s) for every 1 algonauts batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
