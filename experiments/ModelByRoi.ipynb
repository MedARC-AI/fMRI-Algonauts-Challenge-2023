{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e191c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205a4462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# data_dir = '../algonauts_2023_challenge_data'\n",
    "# parent_submission_dir = 'algonauts_2023_challenge_submission'\n",
    "data_dir = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/dataset\"\n",
    "parent_submission_dir = \"/fsx/proj-medarc/fmri/dweisberg/fMRI-Algonauts-Challenge-2023/algonauts_2023_challenge_submission\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "# def seed_everything(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "# seed_everything()\n",
    "\n",
    "subj=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c94b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cdd3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcf2186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    }
   ],
   "source": [
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists with all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f07ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stimulus images: 8700\n",
      "\n",
      "Validation stimulus images: 1141\n",
      "\n",
      "Test  stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "rand_seed = 5 \n",
    "np.random.seed(rand_seed)\n",
    "batch_size = 300\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "num_train = num_train - num_train % batch_size\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "# np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest  stimulus images: ' + format(len(idxs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abee82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d0c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img).to(device)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11d45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "subm_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "    batch_size=batch_size, drop_last=True\n",
    ")\n",
    "val_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_val, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(subm_imgs_paths, idxs_test, transform), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab644601",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "lh_fmri_test = lh_fmri[idxs_test]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]\n",
    "rh_fmri_test = rh_fmri[idxs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de1fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d0d552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /admin/home-dweisberg/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet')\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it\n",
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08673b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=100, batch_size=batch_size)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d50296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b2fae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_list = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"nsdgeneral\"]\n",
    "\n",
    "# Load the ROI classes mapping dictionaries\n",
    "roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n",
    "    'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n",
    "    'mapping_floc-words.npy', 'mapping_streams.npy']\n",
    "roi_name_maps = []\n",
    "for r in roi_mapping_files:\n",
    "    roi_name_maps.append(np.load(os.path.join(args.data_dir, 'roi_masks', r),\n",
    "        allow_pickle=True).item())\n",
    "\n",
    "# Load the ROI brain surface maps\n",
    "lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n",
    "    'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n",
    "    'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n",
    "    'lh.streams_challenge_space.npy']\n",
    "rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n",
    "    'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n",
    "    'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n",
    "    'rh.streams_challenge_space.npy']\n",
    "\n",
    "lh_challenge_rois = []\n",
    "rh_challenge_rois = []\n",
    "\n",
    "for r in range(len(lh_challenge_roi_files)):\n",
    "    lh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n",
    "        lh_challenge_roi_files[r])))\n",
    "    rh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n",
    "        rh_challenge_roi_files[r])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c41a14",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1344472444.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [16]\u001b[0;36m\u001b[0m\n\u001b[0;31m    fmri_lh_val_pred_best =\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# correlations for each roi for each layer, for both hemispheres\n",
    "layer_correlations_lh = np.zeros((len(train_nodes), len(roi_list)))\n",
    "layer_correlations_rh = np.zeros((len(train_nodes), len(roi_list)))\n",
    "\n",
    "roi_model = np.zeros(2, len(roi_list)) # best layer for each roi in each hemisphere\n",
    "fmri_lh_val_pred_best = \n",
    "fmri_rh_val_pred_best\n",
    "fmri_lh_test_pred\n",
    "fmri_rh_test_pred\n",
    "\n",
    "\n",
    "for layer_num, model_layer in enumerate(train_nodes):\n",
    "    print(model_layer)\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet')\n",
    "    model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "    model.eval() # set the model to evaluation mode, since you are not training it\n",
    "    \n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])\n",
    "\n",
    "    pca = fit_pca(feature_extractor, train_imgs_dataloader)\n",
    "\n",
    "    features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "    features_val = extract_features(feature_extractor, val_imgs_dataloader, pca)\n",
    "    features_test = extract_features(feature_extractor, test_imgs_dataloader, pca)\n",
    "\n",
    "    del model, pca\n",
    "\n",
    "    # Fit linear regressions on the training data\n",
    "    reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "    reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "    # Use fitted linear regressions to predict the validation and test fMRI data\n",
    "    lh_fmri_val_pred = reg_lh.predict(features_val)\n",
    "    rh_fmri_val_pred = reg_rh.predict(features_val)\n",
    "\n",
    "    # Empty correlation array of shape: (LH vertices)\n",
    "    lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "    # Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "    for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n",
    "        lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "    # Empty correlation array of shape: (RH vertices)\n",
    "    rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "    # Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "    for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n",
    "        rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "\n",
    "    # Select the correlation results vertices of each ROI\n",
    "    roi_names = []\n",
    "    lh_roi_correlation = []\n",
    "    rh_roi_correlation = []\n",
    "    for r1 in range(len(lh_challenge_rois)):\n",
    "        for r2 in roi_name_maps[r1].items():\n",
    "            if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
    "                roi_names.append(r2[1])\n",
    "                lh_roi_idx = np.where(lh_challenge_rois[r1] == r2[0])[0]\n",
    "                rh_roi_idx = np.where(rh_challenge_rois[r1] == r2[0])[0]\n",
    "                lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
    "                rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
    "    roi_names.append('nsdgeneral')\n",
    "    lh_roi_correlation.append(lh_correlation)\n",
    "    rh_roi_correlation.append(rh_correlation)\n",
    "    \n",
    "    # correlations for layer for all roi's\n",
    "    layer_correlations_lh[layer_num, :] = [np.median(lh_roi_correlation[r]) for r in range(len(lh_roi_correlation))]\n",
    "    layer_correlations_rh[layer_num, :] = [np.median(rh_roi_correlation[r]) for r in range(len(rh_roi_correlation))]\n",
    "    \n",
    "#     layer_correlations_lh[np.isnan(layer_correlations_lh)]=-1\n",
    "#     layer_correlations_rh[np.isnan(layer_correlations_rh)]=-1\n",
    "    \n",
    "#     # for each new layer, for all roi's, test if new correlations beat the old best one\n",
    "#     for \n",
    "#     roi_model[0] = np.argmax(layer_correlations_lh, axis=0)\n",
    "#     roi_model[1] = np.argmax(layer_correlations_rh, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b6f7f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_correlations_lh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.30\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plt.plot(x - width/2, feature_correlations_lh, width, label=roi_list)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m lines \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mplot(x, \u001b[43mlayer_correlations_lh\u001b[49m[:,first\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:last], width,label\u001b[38;5;241m=\u001b[39mroi_list[first\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:last])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lines))\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlim(left\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(x), right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(x)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_correlations_lh' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "x = np.arange(len(train_nodes))\n",
    "first = 1 # min is 1\n",
    "last = 32 # max 32\n",
    "\n",
    "width = 0.30\n",
    "# plt.plot(x - width/2, feature_correlations_lh, width, label=roi_list)\n",
    "lines = plt.plot(x, layer_correlations_lh[:,first-1:last], width,label=roi_list[first-1:last])\n",
    "print(len(lines))\n",
    "plt.xlim(left=min(x), right=max(x)+3.1)\n",
    "# plt.ylim(bottom=0, top=1)\n",
    "plt.xlabel('Layers')\n",
    "plt.xticks(ticks=x, labels=train_nodes, rotation=60)\n",
    "plt.ylabel('Median Pearson\\'s $r$')\n",
    "plt.legend(handles = lines[first-1:last],frameon=True, loc='lower right', ncol = 2)\n",
    "plt.title(\"Correlations per layer by ROI\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n",
    "# rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(args.subject_submission_dir, 'layer_correlations_lh.npy'), layer_correlations_lh)\n",
    "np.save(os.path.join(args.subject_submission_dir, 'layer_correlations_rh.npy'), layer_correlations_rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_correlations_lh_load = np.load(os.path.join(args.subject_submission_dir, 'layer_correlations_lh.npy'))\n",
    "layer_correlations_rh_load = np.load(os.path.join(args.subject_submission_dir, 'layer_correlations_rh.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test whether correlations saved\n",
    "print(np.sum(layer_correlations_lh-layer_correlations_lh_load))\n",
    "print(np.sum(layer_correlations_rh-layer_correlations_rh_load))\n",
    "# print(layer_correlations_lh)\n",
    "print(layer_correlations_rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1943eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_model = np.zeros(2, len(roi_list)) # left and right hemisphere\n",
    "layer_correlations_lh[np.isnan(layer_correlations_lh)]=-1\n",
    "layer_correlations_rh[np.isnan(layer_correlations_rh)]=-1\n",
    "\n",
    "# best layer for each roi in each hemisphere\n",
    "roi_model[0] = np.argmax(layer_correlations_lh, axis=0)\n",
    "roi_model[1] = np.argmax(layer_correlations_rh, axis=0)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
