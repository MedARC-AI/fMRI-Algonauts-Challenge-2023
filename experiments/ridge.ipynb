{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22895cd8",
   "metadata": {},
   "source": [
    "# Import packages + functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712b213c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "from subprocess import call\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc04fa1",
   "metadata": {},
   "source": [
    "# Load NSD webdatasets for a given subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6ef00",
   "metadata": {},
   "source": [
    "The '/fsx' directory refers to a directory housed on the Stability HPC. You will need to download NSD data from [our huggingface dataset](https://huggingface.co/datasets/pscotti/naturalscenesdataset/tree/main) and change the folders below to your own folder path. If you have demonstrated serious involvement with the project and need access to the Stability HPC contact Paul Scotti on Discord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbc9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # alexnet requirement\n",
    "])\n",
    "\n",
    "def preprocess(sample):\n",
    "    voxel, image = sample\n",
    "    return voxel, transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67101d92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_devices 1\n",
      "num_workers 1\n",
      "batch_size 300\n",
      "global_batch_size 300\n",
      "num_worker_batches 30\n",
      "validation: num_worker_batches 1\n"
     ]
    }
   ],
   "source": [
    "subject = '01'\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: # not using gpu, assuming you want to use the limited dataset\n",
    "    num_devices = 1\n",
    "    num_workers = 1\n",
    "    batch_size = 300\n",
    "    num_samples = 9000\n",
    "else: # using cuda, assuming you want to use the full dataset\n",
    "    num_workers = num_devices\n",
    "    batch_size = 300\n",
    "    num_samples = 9000 # see metadata.json\n",
    "print(\"num_devices\",num_devices)\n",
    "print(\"num_workers\",num_workers)\n",
    "print(\"batch_size\",batch_size)\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"global_batch_size\",global_batch_size)\n",
    "num_batches = math.floor(num_samples / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "print(\"num_worker_batches\",num_worker_batches)\n",
    "# train_url = f'/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/train/train_subj{subject}'\n",
    "train_url = f'/scratch/gpfs/KNORMAN/webdataset_nsd/webdataset_split/train/train_subj{subject}'\n",
    "train_url = train_url+\"_{0..49}.tar\"\n",
    "\n",
    "train_data = wds.DataPipeline([wds.ResampledShards(train_url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    #wds.shuffle(500,initial=500),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", 'images'),\n",
    "                    wds.map(preprocess),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=None, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Validation #\n",
    "# num_samples = 492\n",
    "num_samples = 300\n",
    "num_batches = math.ceil(num_samples / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "print(\"validation: num_worker_batches\",num_worker_batches)\n",
    "\n",
    "# val_url = f'/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/val/val_subj{subject}_0.tar'\n",
    "val_url = f'/scratch/gpfs/KNORMAN/webdataset_nsd/webdataset_split/val/val_subj{subject}_0.tar'\n",
    "val_data = wds.DataPipeline([wds.SimpleShardList(val_url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", 'images'),\n",
    "                    wds.map(preprocess),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7cbd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([300, 15724]) tensor([ 1.0732, -0.1417, -0.8462,  0.4048], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([-2.0152, -1.9295, -1.8097, -1.7925])\n",
      "idx 1\n",
      "voxel.shape torch.Size([300, 15724]) tensor([-0.0723, -0.9531, -1.3066, -2.9062], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([ 0.8276, -0.3883, -0.5082, -0.6452])\n",
      "idx 2\n",
      "voxel.shape torch.Size([300, 15724]) tensor([ 0.5205, -0.2817, -1.7275,  0.3120], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([1.8722, 1.8550, 1.8037, 1.8037])\n",
      "idx 3\n",
      "voxel.shape torch.Size([300, 15724]) tensor([ 0.0998,  0.3765, -0.5747, -0.2568], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([-0.6452, -0.7308, -0.7650, -0.8335])\n"
     ]
    }
   ],
   "source": [
    "# check that your training data loaders are working\n",
    "for train_i, (voxel, img_input) in enumerate(train_dl):\n",
    "    print(\"idx\",train_i)\n",
    "    print(\"voxel.shape\",voxel.shape,voxel[0,20:24])\n",
    "    print(\"img_input.shape\",img_input.shape,img_input[0,0,0,100:104])\n",
    "    if train_i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf574d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([300, 15724]) tensor([ 1.0420,  1.2803, -0.4688,  1.0654], dtype=torch.float16)\n",
      "img_input.shape torch.Size([300, 3, 256, 256]) tensor([-0.1999, -0.0972, -0.0801, -0.0458])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check that validation data loader is working\n",
    "for val_i, (voxel, img_input) in enumerate(tqdm(val_dl)):\n",
    "    print(\"idx\",val_i)\n",
    "    print(\"voxel.shape\",voxel.shape,voxel[0,20:24])\n",
    "    print(\"img_input.shape\",img_input.shape,img_input[0,0,0,100:104])\n",
    "    if train_i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22bfa13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# check validation batches are working correctly\n",
    "# nothing prints out if correct\n",
    "for train_i, (voxel, img_input) in enumerate(tqdm(val_dl)):\n",
    "    if voxel.shape[0]!=batch_size:\n",
    "        print(\"idx\",train_i)\n",
    "        print(\"voxel.shape\",voxel.shape,voxel[0,20:24])\n",
    "        print(\"img_input.shape\",img_input.shape,img_input[0,0,0,100:104])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40874f",
   "metadata": {},
   "source": [
    "# Train model (input = image; output = predicted fMRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a408b",
   "metadata": {},
   "source": [
    "### load pretrained alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "728fd753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dw26/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet') # run in commandline if not loading\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6cc76",
   "metadata": {},
   "source": [
    "### Extract and downsample chosen alexnet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50470ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_layer = \"features.5\"  # choose layer\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b355bb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor):\n",
      "    features_0 = getattr(self.features, \"0\")(x);  x = None\n",
      "    features_1 = getattr(self.features, \"1\")(features_0);  features_0 = None\n",
      "    features_2 = getattr(self.features, \"2\")(features_1);  features_1 = None\n",
      "    features_3 = getattr(self.features, \"3\")(features_2);  features_2 = None\n",
      "    features_4 = getattr(self.features, \"4\")(features_3);  features_3 = None\n",
      "    features_5 = getattr(self.features, \"5\")(features_4);  features_4 = None\n",
      "    return {'features.5': features_5}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b72a0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental PCA computes features\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=100, batch_size=batch_size)\n",
    "    # Fit PCA to batch\n",
    "    for _, (voxel, img) in enumerate(tqdm(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(img.to(device))\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b40875f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mfit_pca\u001b[49m(feature_extractor, train_dl)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit_pca' is not defined"
     ]
    }
   ],
   "source": [
    "pca = fit_pca(feature_extractor, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets principal components of features for each images using derived PCA\n",
    "# while going through dataset, also prepare training/validation fMRI matrix from dataloader for regression\n",
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    fmri = torch.tensor([])\n",
    "    for _, (voxel, img) in enumerate(tqdm(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(img.to(device))\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "        \n",
    "        fmri = torch.cat((fmri, voxel), 0)\n",
    "    return np.vstack(features), fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cae371",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, fmri_train = extract_features(feature_extractor, train_dl, pca)\n",
    "features_val, fmri_val = extract_features(feature_extractor, val_dl, pca)\n",
    "\n",
    "print('\\nTraining images features:')\n",
    "print(features_train.shape)\n",
    "print('(Training stimulus images × PCA features)')\n",
    "\n",
    "print('\\nValidation images features:')\n",
    "print(features_val.shape)\n",
    "print('(Validation stimulus images × PCA features)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921fa33",
   "metadata": {},
   "source": [
    "### mask for nsdgeneral voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde86c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# whole brain space is 81x104x83\n",
    "# nsdgeneral sapce is 15724\n",
    "import nibabel as nib\n",
    "\n",
    "file = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/subj{subject}/func1pt8mm/roi/nsdgeneral.nii.gz\"\n",
    "nifti = nib.load(file)\n",
    "nsd_mask = nifti.get_fdata()\n",
    "nsd_mask = nsd_mask>0\n",
    "nsd_mask = torch.from_numpy(nsd_mask)\n",
    "print(nsd_mask.shape)\n",
    "print(torch.sum(nsd_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa1d5e",
   "metadata": {},
   "source": [
    "### Linearly map downsampled alexnet features to fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fMRI training shape:\",fmri_train.shape)\n",
    "print(\"fMRI validation shape:\",fmri_val.shape)\n",
    "print(\"Linearized Features shape:\", features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Fit linear regressions on the training data\n",
    "reg = Ridge().fit(features_train, fmri_train)\n",
    "# Use fitted linear regressions to predict the validation and test fMRI data\n",
    "fmri_val_pred = reg.predict(features_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri_val_pred.shape)\n",
    "print(fmri_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d165fe",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr as corr\n",
    "\n",
    "correlation = np.zeros(fmri_val_pred.shape[1])\n",
    "# Correlate each predicted voxel with the corresponding ground truth voxel\n",
    "for v in tqdm(range(fmri_val_pred.shape[1])):\n",
    "    correlation[v] = corr(fmri_val_pred[:,v], fmri_val[:,v])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation.shape)\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get roi class for given roi\n",
    "def getRoiClass(roi):\n",
    "    if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "        roi_class = 'prf-visualrois'\n",
    "    elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "        roi_class = 'floc-bodies'\n",
    "    elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "        roi_class = 'floc-faces'\n",
    "    elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "        roi_class = 'floc-places'\n",
    "    elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "        roi_class = 'floc-words'\n",
    "    elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "        roi_class = 'streams'\n",
    "    elif roi in [\"nsdgeneral\"]:\n",
    "        roi_class = 'nsdgeneral'\n",
    "    else:\n",
    "        raise ValueError('invalid ROI')\n",
    "    return roi_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e70f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns nsdgeneral roi mask\n",
    "def getRoiMask(hemisphere, roi):\n",
    "    # get ROI class for given ROI\n",
    "    roi_class = getRoiClass(roi)\n",
    "    file_roi = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/subj{subject}/func1pt8mm/roi/{hemisphere[0]}h.{roi_class}.nii.gz\"\n",
    "    nifti_roi = nib.load(file_roi)\n",
    "    mask_roi = nifti_roi.get_fdata()\n",
    "    \n",
    "    # get integer indices for ROI's mask in each ROI class\n",
    "    file_map = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/freesurfer/subj{subject}/label/{roi_class}.mgz.ctab\"\n",
    "    with open(file_map, \"r\") as file1:\n",
    "        map_list = file1.readlines()\n",
    "        map_dict = {}\n",
    "        for label in map_list:\n",
    "            roi_label = label.split()\n",
    "            map_dict.update({roi_label[1]: int(roi_label[0])})\n",
    "\n",
    "    label_int = map_dict[roi]\n",
    "    mask_roi[mask_roi!=label_int] = 0\n",
    "    mask_roi = np.asarray(mask_roi,bool)\n",
    "    mask_roi = mask_roi[nsd_mask==1]\n",
    "    \n",
    "    return mask_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation by ROI\n",
    "roi_list = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"nsdgeneral\"]\n",
    "\n",
    "lh_roi_correlation = []\n",
    "rh_roi_correlation = []\n",
    "for hemisphere in ['left', 'right']:\n",
    "    for roi in tqdm(roi_list):\n",
    "        mask_roi = getRoiMask(hemisphere,roi)\n",
    "        correlation_roi = correlation[mask_roi==1]\n",
    "\n",
    "        if hemisphere == 'left':\n",
    "            lh_roi_correlation.append(correlation_roi)\n",
    "        else:\n",
    "            rh_roi_correlation.append(correlation_roi)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "lh_median_roi_correlation = [np.median(lh_roi_correlation[r]) for r in range(len(lh_roi_correlation))]\n",
    "rh_median_roi_correlation = [np.median(rh_roi_correlation[r]) for r in range(len(rh_roi_correlation))]\n",
    "plt.figure(figsize=(18,6))\n",
    "x = np.arange(len(roi_list))\n",
    "width = 0.30\n",
    "plt.bar(x - width/2, lh_median_roi_correlation, width, label='Left Hemisphere')\n",
    "plt.bar(x + width/2, rh_median_roi_correlation, width,label='Right Hemishpere')\n",
    "plt.xlim(left=min(x)-.5, right=max(x)+.5)\n",
    "plt.ylim(bottom=0, top=1)\n",
    "plt.xlabel('ROIs')\n",
    "plt.xticks(ticks=x, labels=roi_list, rotation=60)\n",
    "plt.ylabel('Median Pearson\\'s $r$')\n",
    "plt.legend(frameon=True, loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd678bf",
   "metadata": {},
   "source": [
    "# Map from native voxel space to fsaverage surface space\n",
    "The Algonauts competition requires results in fsaverage space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f2b43",
   "metadata": {},
   "source": [
    "## Save voxel predictions as 3d Nifti file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85519491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation.shape)\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7b927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn.image import load_img, new_img_like\n",
    "# temp_s3_dir = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3'\n",
    "temp_s3_dir = '/scratch/gpfs/KNORMAN/natural-scenes-dataset'\n",
    "\n",
    "roi = roi_list[7]\n",
    "print(roi)\n",
    "\n",
    "mask_roi = getRoiMask('left',roi)\n",
    "print(mask_roi.shape)\n",
    "print(np.sum(mask_roi.astype(int)))\n",
    "# data_to_convert = np.copy(fmri_val_pred[0]) # example activation\n",
    "data_to_convert = np.copy(correlation) # correlations\n",
    "\n",
    "data_to_convert[mask_roi==False]=0\n",
    "mask = torch.clone(nsd_mask).numpy()\n",
    "mask[mask==True]=data_to_convert\n",
    "data_to_convert = mask\n",
    "data_to_convert = np.array(data_to_convert).astype(np.float32)\n",
    "# data_to_convert = np.moveaxis(data_to_convert,0,-1)\n",
    "\n",
    "out_niimg_path = 'out.nii.gz'\n",
    "avg_out_niimg_path = 'avg_out.nii.gz'\n",
    "\n",
    "ref_niimg = load_img(f'{temp_s3_dir}/nsddata_betas/ppdata/subj{subject}/func1pt8mm/betas_fithrf_GLMdenoise_RR/R2_session01.nii.gz')\n",
    "assert ref_niimg.get_fdata().shape == data_to_convert.shape[:3]\n",
    "\n",
    "out_niimg = new_img_like(ref_niimg, data_to_convert, affine=None, copy_header=True)\n",
    "out_niimg.to_filename(out_niimg_path)\n",
    "print(out_niimg.shape)\n",
    "\n",
    "# also output a nifti volume averaging across the first dimension\n",
    "# avg_out_niimg = new_img_like(ref_niimg, np.mean(data_to_convert,axis=3), affine=None, copy_header=True)\n",
    "avg_out_niimg = new_img_like(ref_niimg, np.mean(data_to_convert,axis=0), affine=None, copy_header=True)\n",
    "avg_out_niimg.to_filename(avg_out_niimg_path)\n",
    "print(avg_out_niimg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a752b4-b8f8-48af-8d9d-8a34faad7377",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize brain volume (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb82a12-b860-410d-82e7-acaa277545fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "anat = load_img(f'{temp_s3_dir}/nsddata/ppdata/subj01/anat/T1_0pt8_masked.nii.gz')\n",
    "# plotting.plot_anat(anat).add_overlay(avg_out_niimg,cmap=plotting.cm.cold_hot,clim=[-3,3])\n",
    "plotting.plot_anat(anat).add_overlay(out_niimg,cmap=plotting.cm.cold_hot,clim=[-3,3])\n",
    "# plotting.view_img(avg_out_niimg,bg_img=anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f5f6c",
   "metadata": {},
   "source": [
    "## Convert Nifti from native voxel space -> native surface space -> fsaverage surface space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b3626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nsdcode.nsd_mapdata import NSDmapdata\n",
    "from nsdcode.nsd_datalocation import nsd_datalocation\n",
    "nsd = NSDmapdata(temp_s3_dir)\n",
    "\n",
    "# Example terminal commands to cp NSD buckets temporarily to local storage \n",
    "# remove \"--recursive\" if copying a single file instead of contents of a folder\n",
    "# in_dir = 'nsddata/ppdata/subj01/transforms/lh.MNI-to-layerB3.mgz'\n",
    "# print(f\"aws s3 cp --recursive s3://natural-scenes-dataset/{in_dir} {temp_s3_dir}/{in_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b969c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to subject-specific surface space across the three cortical depths\n",
    "for hemisphere in ['lh','rh']:\n",
    "    data = []\n",
    "    for p in range(3):\n",
    "        data.append(\n",
    "            nsd.fit(\n",
    "                int(subject),\n",
    "                'func1pt8', # source space\n",
    "                f'{hemisphere}.layerB{p+1}', # target space\n",
    "                out_niimg_path,\n",
    "                'cubic',\n",
    "                badval=0\n",
    "            )\n",
    "        )\n",
    "    data = np.array(data)\n",
    "    assert np.all(data==0) == False\n",
    "\n",
    "    # Now we average results across the three cortical depths and use\n",
    "    # nearest-neighbor interpolation to bring the result to fsaverage.\n",
    "    surface_data = nsd.fit(\n",
    "        int(subject),\n",
    "        f'{hemisphere}.white',\n",
    "        'fsaverage',\n",
    "        np.mean(data, axis=0),\n",
    "        interptype=None,\n",
    "        badval=0,\n",
    "        fsdir=f'{temp_s3_dir}/nsddata/freesurfer/subj01')\n",
    "    if hemisphere=='lh':\n",
    "        lh_surface_data = surface_data\n",
    "    elif hemisphere=='rh':\n",
    "        rh_surface_data = surface_data\n",
    "    print(f\"hemisphere {hemisphere}: surface_data.shape=\", surface_data.shape)\n",
    "    \n",
    "lh_surface_data = lh_surface_data.astype(np.float32)\n",
    "rh_surface_data = rh_surface_data.astype(np.float32)\n",
    "\n",
    "assert not np.all(lh_surface_data == rh_surface_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551588c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8553c",
   "metadata": {},
   "source": [
    "## Visualize surface activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7ef8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the interactive brain surface map\n",
    "from nilearn import datasets\n",
    "for hemisphere in ['left','right']:\n",
    "    if hemisphere=='left':\n",
    "        surface_data = lh_surface_data\n",
    "    elif hemisphere=='right':\n",
    "        surface_data = rh_surface_data\n",
    "    # fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "    fsaverage = datasets.fetch_surf_fsaverage('fsaverage', data_dir='/scratch/gpfs/dw26')\n",
    "    print(surface_data.shape)\n",
    "    plotting.plot_surf(\n",
    "        surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "#         surf_map=np.mean(surface_data,axis=1),\n",
    "        surf_map = surface_data,\n",
    "        bg_map=fsaverage['sulc_'+hemisphere],\n",
    "        hemi=hemisphere,\n",
    "        view='lateral',\n",
    "        threshold=1e-14,\n",
    "        cmap='cold_hot',\n",
    "        colorbar=True,\n",
    "        title='All vertices, '+hemisphere+' hemisphere'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40251134",
   "metadata": {},
   "source": [
    "# Save results in a way compatible with Algonauts submission\n",
    "After predicted data of all subjects and hemispheres have been saved, zip the parent submission folder (algonauts_2023_challenge_submission), which can then be submitted to CodaLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db81bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f'algonauts_2023_challenge_submission/subj{subject}/lh_pred_test.npy',lh_surface_data)\n",
    "np.save(f'algonauts_2023_challenge_submission/subj{subject}/rh_pred_test.npy',rh_surface_data)\n",
    "print(\"Saved predictions!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
